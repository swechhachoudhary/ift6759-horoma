{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import logging\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "import torchvision.datasets\n",
    "\n",
    "from utils import data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import shutil\n",
    "import _pickle as cPickle\n",
    "from sklearn import preprocessing\n",
    "import subprocess\n",
    "\n",
    "from IPython.core.debugger import Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.seed_val = 0 # random seed val\n",
    "        self.num_train_sup = 4000 # number of labeled train samples\n",
    "        self.batch_size = 100 # batch size\n",
    "        self.labeled_batch_size = 50 # number of labeled samples in a batch\n",
    "        self.device = 0 # gpu id\n",
    "        \n",
    "        self.lr = [0.001, 0.15] # learning rate for adam and then and then initial learning rate for sgd respectively\n",
    "        self.num_epochs = 500 # number of training epochs\n",
    "        self.weight_decay = 5e-4 # weight decay\n",
    "        \n",
    "        self.mount_point = '/tan' # change this to your mount_point\n",
    "        self.datadir = 'data-local/images/cifar/cifar10/by-image' # dataset directory\n",
    "        self.labels = 'data-local/labels/cifar10/4000_balanced_labels/00.txt' # label directory\n",
    "        self.log_dir = os.path.join(self.mount_point,'logs') # log directory\n",
    "        self.model_dir = os.path.join(self.mount_point,'models') # log \n",
    "        self.exp_name = 'cifar10_nl_%i_allconv13_seed_%i'%(self.num_train_sup, self.seed_val) # name of experiments\n",
    "        \n",
    "        self.train_subdir = 'train+val'\n",
    "        self.eval_subdir = 'test'\n",
    "        self.num_classes = 10\n",
    "        self.workers = 4\n",
    "        \n",
    "        self.alpha_reconst = 0.5 # weight for reconstruction loss\n",
    "        self.alpha_pn = 1.0 # weight for path normalization loss\n",
    "        self.alpha_kl = 0.5 # weight for kl loss\n",
    "        self.alpha_bnmm = 0.5 # weight for moment matching loss when doing batchnorm\n",
    "        \n",
    "        self.use_bias = True # add bias after batchnorm\n",
    "        self.use_bn = True # use batch norm\n",
    "        self.do_topdown = True # do topdown\n",
    "        self.do_pn = True # do path normalization\n",
    "        self.do_bnmm = True # do moment matching for batchnorm\n",
    "\n",
    "opt = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "th.cuda.set_device(opt.device)\n",
    "device = th.device(\"cuda:%i\"%opt.device if (opt.device >= 0) else \"cpu\")\n",
    "\n",
    "def gpu_device(device=0):\n",
    "    try:\n",
    "        _ = th.tensor([1, 2, 3], device=th.device('cuda', device))\n",
    "    except ValueError:\n",
    "        return None\n",
    "    return th.device('cuda', device)\n",
    "\n",
    "assert gpu_device(opt.device), 'No GPU device found!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make required folders\n",
    "if not os.path.exists(opt.log_dir):\n",
    "    os.makedirs(opt.log_dir)\n",
    "log_dir = os.path.join(opt.log_dir, opt.exp_name)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "if not os.path.exists(opt.model_dir):\n",
    "    os.makedirs(opt.model_dir)\n",
    "if not os.path.exists(os.path.join(opt.mount_point,'datasets')):\n",
    "    os.makedirs(os.path.join(opt.mount_point,'datasets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-04 09:20:52,573 - <__main__.Options object at 0x7f1e957f3940>\n"
     ]
    }
   ],
   "source": [
    "# set logging option\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "console = logging.StreamHandler()\n",
    "console.setFormatter(formatter)\n",
    "logger.addHandler(console)\n",
    "\n",
    "hdlr = logging.FileHandler(os.path.join(opt.log_dir, '{}.log'.format(opt.exp_name)))\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logging.info(opt)\n",
    "\n",
    "writer = SummaryWriter(os.path.join(opt.log_dir, opt.exp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data loaders\n",
    "channel_stats = dict(mean=[0.4914, 0.4822, 0.4465],\n",
    "                         std=[0.2470,  0.2435,  0.2616])\n",
    "train_transformation = transforms.Compose([\n",
    "    data.RandomTranslateWithReflect(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(**channel_stats)\n",
    "])\n",
    "eval_transformation = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(**channel_stats)\n",
    "])\n",
    "\n",
    "traindir = os.path.join(opt.datadir, opt.train_subdir)\n",
    "evaldir = os.path.join(opt.datadir, opt.eval_subdir)\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(traindir, train_transformation)\n",
    "\n",
    "with open(opt.labels) as f:\n",
    "    labels = dict(line.split(' ') for line in f.read().splitlines())\n",
    "labeled_idxs, unlabeled_idxs = data.relabel_dataset(dataset, labels)\n",
    "\n",
    "batch_sampler = data.TwoStreamBatchSampler(unlabeled_idxs, labeled_idxs, opt.batch_size, opt.labeled_batch_size)\n",
    "\n",
    "train_loader = th.utils.data.DataLoader(dataset,\n",
    "                                        batch_sampler=batch_sampler,\n",
    "                                        num_workers=opt.workers,\n",
    "                                        pin_memory=True)\n",
    "\n",
    "eval_loader = th.utils.data.DataLoader(\n",
    "    torchvision.datasets.ImageFolder(evaldir, eval_transformation),\n",
    "    batch_size=opt.batch_size//2,\n",
    "    shuffle=False,\n",
    "    num_workers=2 * opt.workers,  # Needs images twice as fast\n",
    "    pin_memory=True,\n",
    "    drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# set losses\n",
    "NO_LABEL = -1\n",
    "criterion = nn.CrossEntropyLoss(size_average=False, ignore_index=NO_LABEL).cuda()\n",
    "L2_loss = nn.MSELoss(size_average=False, reduce=False, reduction='mean').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the NRM\n",
    "from nrm import NRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some util functions\n",
    "def get_acc(output, label):\n",
    "    pred = th.argmax(output, dim=1, keepdim=False)\n",
    "    correct = th.mean((pred == label).type(th.FloatTensor))\n",
    "    return correct\n",
    "\n",
    "class AverageMeterSet:\n",
    "    def __init__(self):\n",
    "        self.meters = {}\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.meters[key]\n",
    "\n",
    "    def update(self, name, value, n=1):\n",
    "        if not name in self.meters:\n",
    "            self.meters[name] = AverageMeter()\n",
    "        self.meters[name].update(value, n)\n",
    "\n",
    "    def reset(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.reset()\n",
    "\n",
    "    def values(self, postfix=''):\n",
    "        return {name + postfix: meter.val for name, meter in self.meters.items()}\n",
    "\n",
    "    def averages(self, postfix='/avg'):\n",
    "        return {name + postfix: meter.avg for name, meter in self.meters.items()}\n",
    "\n",
    "    def sums(self, postfix='/sum'):\n",
    "        return {name + postfix: meter.sum for name, meter in self.meters.items()}\n",
    "\n",
    "    def counts(self, postfix='/count'):\n",
    "        return {name + postfix: meter.count for name, meter in self.meters.items()}\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __format__(self, format):\n",
    "        return \"{self.val:{format}} ({self.avg:{format}})\".format(self=self, format=format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training loop\n",
    "def train(net, train_loader, eval_loader, num_epochs, wd):\n",
    "    trainer = th.optim.Adam(net.parameters(), opt.lr[0], weight_decay=wd)\n",
    "    \n",
    "    prev_time = datetime.datetime.now()\n",
    "    best_valid_acc = 0\n",
    "    iter_indx = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0; train_loss_xentropy = 0; train_loss_reconst = 0; train_loss_pn = 0; train_loss_kl = 0; train_loss_bnmm = 0\n",
    "        correct = 0\n",
    "        num_batch_train = 0\n",
    "        \n",
    "        # start with adam optimizer but switch sgd optimizer with exponential decay learning rate since epoch 20 \n",
    "        if epoch == 20:\n",
    "            sgd_lr = opt.lr[1]\n",
    "            decay_val = np.exp(np.log(sgd_lr / 0.0001) / (num_epochs - 2))\n",
    "            sgd_lr = sgd_lr * decay_val\n",
    "            trainer = th.optim.SGD(net.parameters(), sgd_lr, weight_decay=wd)\n",
    "            \n",
    "        if epoch >= 20:\n",
    "            for param_group in trainer.param_groups:\n",
    "                param_group['lr'] = param_group['lr']/decay_val\n",
    "                \n",
    "        for param_group in trainer.param_groups:\n",
    "            learning_rate = param_group['lr']\n",
    "        \n",
    "        meters = AverageMeterSet()\n",
    "        \n",
    "        # switch to train mode\n",
    "        net.train()\n",
    "        \n",
    "        end = time.time()\n",
    "        for i, (batch, target) in enumerate(train_loader):\n",
    "            meters.update('data_time', time.time() - end)\n",
    "            \n",
    "            # set up unlabeled input and labeled input with the corresponding labels\n",
    "            input_unsup_var = th.autograd.Variable(batch[0:(opt.batch_size - opt.labeled_batch_size)]).to(device)\n",
    "            input_sup_var = th.autograd.Variable(batch[(opt.batch_size - opt.labeled_batch_size):]).to(device)\n",
    "            target_sup_var = th.autograd.Variable(target[(opt.batch_size - opt.labeled_batch_size):].cuda(async=True)).to(device)\n",
    "            \n",
    "            minibatch_unsup_size = opt.batch_size - opt.labeled_batch_size\n",
    "            minibatch_sup_size = opt.labeled_batch_size\n",
    "            \n",
    "            # compute loss for unlabeled input\n",
    "            [output_unsup, xhat_unsup, loss_pn_unsup, loss_bnmm_unsup] = net(input_unsup_var)\n",
    "            loss_reconst_unsup = L2_loss(xhat_unsup, input_unsup_var).mean()\n",
    "            softmax_unsup = F.softmax(output_unsup)\n",
    "            loss_kl_unsup = -th.sum(th.log(10.0*softmax_unsup + 1e-8) * softmax_unsup) / minibatch_unsup_size\n",
    "            loss_unsup = opt.alpha_reconst * loss_reconst_unsup + opt.alpha_kl * loss_kl_unsup + opt.alpha_bnmm * loss_bnmm_unsup + opt.alpha_pn * loss_pn_unsup\n",
    "            \n",
    "            # compute loss for labeled input\n",
    "            [output_sup, xhat_sup, loss_pn_sup, loss_bnmm_sup] = net(input_sup_var, target_sup_var)\n",
    "            loss_xentropy_sup = criterion(output_sup, target_sup_var) / minibatch_sup_size\n",
    "            loss_reconst_sup = L2_loss(xhat_sup, input_sup_var).mean()\n",
    "            softmax_sup = F.softmax(output_sup)\n",
    "            loss_kl_sup = -th.sum(th.log(10.0*softmax_sup + 1e-8) * softmax_sup)/ minibatch_sup_size\n",
    "            loss_sup = loss_xentropy_sup + opt.alpha_reconst * loss_reconst_sup + opt.alpha_kl * loss_kl_sup + opt.alpha_bnmm * loss_bnmm_sup + opt.alpha_pn * loss_pn_sup\n",
    "\n",
    "            loss = th.mean(loss_unsup + loss_sup)\n",
    "            \n",
    "            # compute the grads and update the parameters\n",
    "            trainer.zero_grad()\n",
    "            loss.backward()\n",
    "            trainer.step()\n",
    "            \n",
    "            # accumulate all the losses for visualization\n",
    "            loss_reconst = loss_reconst_unsup + loss_reconst_sup\n",
    "            loss_pn = loss_pn_unsup + loss_pn_sup\n",
    "            loss_xentropy = loss_xentropy_sup\n",
    "            loss_kl = loss_kl_unsup + loss_kl_sup\n",
    "            loss_bnmm = loss_bnmm_unsup + loss_bnmm_sup\n",
    "            \n",
    "            train_loss_xentropy += th.mean(loss_xentropy).cpu().detach().numpy()\n",
    "            train_loss_reconst += th.mean(loss_reconst).cpu().detach().numpy()\n",
    "            train_loss_pn += th.mean(loss_pn).cpu().detach().numpy()\n",
    "            train_loss_kl += th.mean(loss_kl).cpu().detach().numpy()\n",
    "            train_loss_bnmm += th.mean(loss_bnmm).cpu().detach().numpy()\n",
    "            train_loss += th.mean(loss).cpu().detach().numpy()\n",
    "            correct += get_acc(output_sup, target_sup_var).cpu().detach().numpy()\n",
    "            \n",
    "            num_batch_train += 1\n",
    "            iter_indx += 1\n",
    "        \n",
    "        writer.add_scalars('loss', {'train': train_loss / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_xentropy', {'train': train_loss_xentropy / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_reconst', {'train': train_loss_reconst / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_pn', {'train': train_loss_pn / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_kl', {'train': train_loss_kl / num_batch_train}, epoch)\n",
    "        writer.add_scalars('loss_bnmm', {'train': train_loss_bnmm / num_batch_train}, epoch)\n",
    "        writer.add_scalars('acc', {'train': correct / num_batch_train}, epoch)\n",
    "        \n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        \n",
    "        # Validation\n",
    "        valid_loss = 0; valid_loss_xentropy = 0; valid_loss_reconst = 0; valid_loss_pn = 0; valid_loss_kl = 0; valid_loss_bnmm = 0\n",
    "        valid_correct = 0\n",
    "        num_batch_valid = 0\n",
    "\n",
    "        net.eval()\n",
    "\n",
    "        for i, (batch, target) in enumerate(eval_loader):\n",
    "            with th.no_grad():\n",
    "                input_var = th.autograd.Variable(batch).to(device)\n",
    "                target_var = th.autograd.Variable(target.cuda(async=True)).to(device)\n",
    "\n",
    "                minibatch_size = len(target_var)\n",
    "\n",
    "                [output, xhat, loss_pn, loss_bnmm] = net(input_var, target_var)\n",
    "\n",
    "                loss_xentropy = criterion(output, target_var)/minibatch_size\n",
    "                loss_reconst = L2_loss(xhat, input_var).mean()\n",
    "                softmax_val = F.softmax(output)\n",
    "                loss_kl = -th.sum(th.log(10.0*softmax_val + 1e-8) * softmax_val)/minibatch_size\n",
    "                loss = loss_xentropy + opt.alpha_reconst * loss_reconst + opt.alpha_kl * loss_kl + opt.alpha_bnmm * loss_bnmm + opt.alpha_pn * loss_pn\n",
    "\n",
    "                valid_loss_xentropy += th.mean(loss_xentropy).cpu().detach().numpy()\n",
    "                valid_loss_reconst += th.mean(loss_reconst).cpu().detach().numpy()\n",
    "                valid_loss_pn += th.mean(loss_pn).cpu().detach().numpy()\n",
    "                valid_loss_kl += th.mean(loss_kl).cpu().detach().numpy()\n",
    "                valid_loss_bnmm += th.mean(loss_bnmm).cpu().detach().numpy()\n",
    "                valid_loss += th.mean(loss).cpu().detach().numpy()\n",
    "                valid_correct += get_acc(output, target_var).cpu().detach().numpy()\n",
    "\n",
    "                num_batch_valid += 1\n",
    "        \n",
    "        valid_acc = valid_correct / num_batch_valid\n",
    "        if valid_acc > best_valid_acc:\n",
    "            best_valid_acc = valid_acc\n",
    "            th.save(net.state_dict(), '%s/%s_best.pth'%(opt.model_dir, opt.exp_name))\n",
    "        writer.add_scalars('loss', {'valid': valid_loss / num_batch_valid}, epoch)\n",
    "        writer.add_scalars('loss_xentropy', {'valid': valid_loss_xentropy / num_batch_valid}, epoch)\n",
    "        writer.add_scalars('loss_reconst', {'valid': valid_loss_reconst / num_batch_valid}, epoch)\n",
    "        writer.add_scalars('loss_pn', {'valid': valid_loss_pn / num_batch_valid}, epoch)\n",
    "        writer.add_scalars('loss_kl', {'valid': valid_loss_kl / num_batch_valid}, epoch)\n",
    "        writer.add_scalars('loss_bnmm', {'valid': valid_loss_bnmm / num_batch_valid}, epoch)\n",
    "        writer.add_scalars('acc', {'valid': valid_acc}, epoch)\n",
    "        epoch_str = (\"Epoch %d. Train Loss: %f, Train Xent: %f, Train Reconst: %f, Train Pn: %f, Train acc %f, Valid Loss: %f, Valid acc %f, Best valid acc %f, \"\n",
    "                     % (epoch, train_loss / num_batch_train, train_loss_xentropy / num_batch_train, train_loss_reconst / num_batch_train, train_loss_pn / num_batch_train,\n",
    "                        correct / num_batch_train, valid_loss / num_batch_valid, valid_acc, best_valid_acc))\n",
    "        if not epoch % 20:\n",
    "            th.save(net.state_dict(), '%s/%s_epoch_%i.pth'%(opt.model_dir, opt.exp_name, epoch))\n",
    "\n",
    "        prev_time = cur_time\n",
    "        logging.info(epoch_str + time_str + ', lr ' + str(learning_rate))\n",
    "        \n",
    "    return best_valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        th.nn.init.xavier_uniform(m.weight)\n",
    "        # m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Bias') != -1:\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "def run_train(num_exp):\n",
    "    valid_acc = 0\n",
    "    for i in range(num_exp):\n",
    "        model = NRM('AllConv13', batch_size=opt.batch_size // 2, num_class=10, use_bias=opt.use_bias, use_bn=opt.use_bn, do_topdown=opt.do_topdown, do_pn=opt.do_pn, do_bnmm=opt.do_bnmm).to(device)\n",
    "        model.apply(weights_init)\n",
    "                \n",
    "        acc = train(model, train_loader, eval_loader, opt.num_epochs, opt.weight_decay)\n",
    "        logging.info('Validation Accuracy - Run %i = %f'%(i, acc))\n",
    "        valid_acc += acc\n",
    "\n",
    "    logging.info('Validation Accuracy = %f'%(valid_acc/num_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:131: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "run_train(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
