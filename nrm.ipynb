{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import logging\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "import torchvision.datasets\n",
    "\n",
    "from utils import data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import shutil\n",
    "import _pickle as cPickle\n",
    "from sklearn import preprocessing\n",
    "import subprocess\n",
    "\n",
    "from IPython.core.debugger import Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.seed_val = 0 # random seed val\n",
    "        self.num_train_sup = 4000 # number of labeled train samples\n",
    "        self.batch_size = 500 # batch size\n",
    "        self.labeled_batch_size = 3 # number of labeled samples in a batch\n",
    "        self.device = 0 # gpu id\n",
    "        \n",
    "        self.lr = [0.001, 0.15] # learning rate for adam and then and then initial learning rate for sgd respectively\n",
    "        self.num_epochs = 500 # number of training epochs\n",
    "        self.weight_decay = 5e-4 # weight decay\n",
    "        \n",
    "        self.mount_point = '/home/onu/Desktop/' # change this to your mount_point\n",
    "        self.datadir = 'data-local/images/cifar/cifar10/by-image' # dataset directory\n",
    "        self.labels = 'data-local/labels/cifar10/4000_balanced_labels/00.txt' # label directory\n",
    "        self.log_dir = os.path.join(self.mount_point,'logs') # log directory\n",
    "        self.model_dir = os.path.join(self.mount_point,'models') # log \n",
    "        self.exp_name = 'cifar10_nl_%i_allconv13_seed_%i'%(self.num_train_sup, self.seed_val) # name of experiments\n",
    "        \n",
    "        self.train_subdir = 'train+val'\n",
    "        self.eval_subdir = 'test'\n",
    "        self.num_classes = 17\n",
    "        self.workers = 4\n",
    "        \n",
    "        self.alpha_reconst = 0.5 # weight for reconstruction loss\n",
    "        self.alpha_pn = 1.0 # weight for path normalization loss\n",
    "        self.alpha_kl = 0.5 # weight for kl loss\n",
    "        self.alpha_bnmm = 0.5 # weight for moment matching loss when doing batchnorm\n",
    "        \n",
    "        self.use_bias = True # add bias after batchnorm\n",
    "        self.use_bn = True # use batch norm\n",
    "        self.do_topdown = True # do topdown\n",
    "        self.do_pn = True # do path normalization\n",
    "        self.do_bnmm = True # do moment matching for batchnorm\n",
    "\n",
    "opt = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "th.cuda.set_device(opt.device)\n",
    "device = th.device(\"cuda:%i\"%opt.device if (opt.device >= 0) else \"cpu\")\n",
    "\n",
    "def gpu_device(device=0):\n",
    "    try:\n",
    "        _ = th.tensor([1, 2, 3], device=th.device('cuda', device))\n",
    "    except ValueError:\n",
    "        return None\n",
    "    return th.device('cuda', device)\n",
    "\n",
    "assert gpu_device(opt.device), 'No GPU device found!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make required folders\n",
    "if not os.path.exists(opt.log_dir):\n",
    "    os.makedirs(opt.log_dir)\n",
    "log_dir = os.path.join(opt.log_dir, opt.exp_name)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "if not os.path.exists(opt.model_dir):\n",
    "    os.makedirs(opt.model_dir)\n",
    "if not os.path.exists(os.path.join(opt.mount_point,'datasets')):\n",
    "    os.makedirs(os.path.join(opt.mount_point,'datasets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-08 21:17:40,755 - <__main__.Options object at 0x7f7e3c0eab00>\n"
     ]
    }
   ],
   "source": [
    "# set logging option\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "console = logging.StreamHandler()\n",
    "console.setFormatter(formatter)\n",
    "logger.addHandler(console)\n",
    "\n",
    "hdlr = logging.FileHandler(os.path.join(opt.log_dir, '{}.log'.format(opt.exp_name)))\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logging.info(opt)\n",
    "\n",
    "writer = SummaryWriter(os.path.join(opt.log_dir, opt.exp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onu/miniconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/onu/miniconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# set losses\n",
    "NO_LABEL = -1\n",
    "criterion = nn.CrossEntropyLoss(size_average=False, ignore_index=NO_LABEL).cuda()\n",
    "L2_loss = nn.MSELoss(size_average=False, reduce=False, reduction='mean').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the NRM\n",
    "from nrm import NRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some util functions\n",
    "def get_acc(output, label):\n",
    "    pred = th.argmax(output, dim=1, keepdim=False)\n",
    "    correct = th.mean((pred == label).type(th.FloatTensor))\n",
    "    return correct\n",
    "\n",
    "class AverageMeterSet:\n",
    "    def __init__(self):\n",
    "        self.meters = {}\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.meters[key]\n",
    "\n",
    "    def update(self, name, value, n=1):\n",
    "        if not name in self.meters:\n",
    "            self.meters[name] = AverageMeter()\n",
    "        self.meters[name].update(value, n)\n",
    "\n",
    "    def reset(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.reset()\n",
    "\n",
    "    def values(self, postfix=''):\n",
    "        return {name + postfix: meter.val for name, meter in self.meters.items()}\n",
    "\n",
    "    def averages(self, postfix='/avg'):\n",
    "        return {name + postfix: meter.avg for name, meter in self.meters.items()}\n",
    "\n",
    "    def sums(self, postfix='/sum'):\n",
    "        return {name + postfix: meter.sum for name, meter in self.meters.items()}\n",
    "\n",
    "    def counts(self, postfix='/count'):\n",
    "        return {name + postfix: meter.count for name, meter in self.meters.items()}\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __format__(self, format):\n",
    "        return \"{self.val:{format}} ({self.avg:{format}})\".format(self=self, format=format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set:  (152228, 3, 32, 32)\n",
      "Shape of validation set:  (228, 3, 32, 32)\n",
      "Shape of validation set:  (252, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "from data.dataset import HoromaDataset\n",
    "\n",
    "from comet_ml import OfflineExperiment\n",
    "import json\n",
    "import argparse\n",
    "from models import *\n",
    "from models.clustering import *\n",
    "from utils.ali_utils import *\n",
    "from utils.utils import *\n",
    "from utils.utils import load_datasets\n",
    "from utils.constants import Constants\n",
    "from data.dataset import HoromaDataset\n",
    "import torch\n",
    "path_to_model = None\n",
    "config_key = 'HALI'\n",
    "config = 'HALI'\n",
    "\n",
    "with open(Constants.CONFIG_PATH, 'r') as f:\n",
    "    configuration = json.load(f)[config_key]\n",
    "\n",
    "# Parse configuration file\n",
    "clustering_model = configuration['cluster_model']\n",
    "encoding_model = configuration['enc_model']\n",
    "batch_size = configuration['batch_size']\n",
    "seed = configuration['seed']\n",
    "n_epochs = configuration['n_epochs']\n",
    "train_subset = configuration['train_subset']\n",
    "train_split = configuration['train_split']\n",
    "valid_split = configuration['valid_split']\n",
    "train_labeled_split = configuration['train_labeled_split']\n",
    "encode = configuration['encode']\n",
    "cluster = configuration['cluster']\n",
    "flattened = False  # Default\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set all seeds for full reproducibility\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "datapath = Constants.DATAPATH\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "train = HoromaDataset(datapath, split=train_split, subset=train_subset,\n",
    "                      flattened=flattened)\n",
    "labeled = HoromaDataset(datapath, split=train_labeled_split, subset=train_subset,\n",
    "                        flattened=flattened)\n",
    "valid_data = HoromaDataset(\n",
    "    datapath, split=valid_split, subset=train_subset, flattened=flattened)\n",
    "\n",
    "train_label_indices = labeled.targets\n",
    "valid_indices = valid_data.targets\n",
    "\n",
    "print(\"Shape of training set: \", train.data.shape)\n",
    "print(\"Shape of validation set: \", labeled.data.shape)\n",
    "print(\"Shape of validation set: \", valid_data.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=opt.batch_size, shuffle=True)\n",
    "labeled_loader = DataLoader(labeled, batch_size=opt.labeled_batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(valid_data, batch_size=opt.labeled_batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.0\n"
     ]
    }
   ],
   "source": [
    "n_iterations = np.floor(train.data.shape[0]/opt.batch_size)\n",
    "print(n_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.labeled_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = th.optim.Adam(net.parameters(), opt.lr[0], weight_decay=opt.weight_decay)\n",
    "\n",
    "prev_time = datetime.datetime.now()\n",
    "best_valid_acc = 0\n",
    "iter_indx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = 0; train_loss_xentropy = 0; train_loss_reconst = 0; train_loss_pn = 0; train_loss_kl = 0; train_loss_bnmm = 0\n",
    "correct = 0\n",
    "num_batch_train = 0\n",
    "\n",
    "# start with adam optimizer but switch sgd optimizer with exponential decay learning rate since epoch 20 \n",
    "if epoch == 20:\n",
    "    sgd_lr = opt.lr[1]\n",
    "    decay_val = np.exp(np.log(sgd_lr / 0.0001) / (num_epochs - 2))\n",
    "    sgd_lr = sgd_lr * decay_val\n",
    "    trainer = th.optim.SGD(net.parameters(), sgd_lr, weight_decay=wd)\n",
    "\n",
    "if epoch >= 20:\n",
    "    for param_group in trainer.param_groups:\n",
    "        param_group['lr'] = param_group['lr']/decay_val\n",
    "\n",
    "for param_group in trainer.param_groups:\n",
    "    learning_rate = param_group['lr']\n",
    "meters = AverageMeterSet()\n",
    "\n",
    "# switch to train mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsup_batch = next(iter(train_loader))\n",
    "sup_batch,target = next(iter(labeled_loader))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up unlabeled input and labeled input with the corresponding labels\n",
    "input_unsup_var = th.autograd.Variable(unsup_batch).to(device)\n",
    "input_sup_var = th.autograd.Variable(sup_batch).to(device)\n",
    "target_sup_var = th.autograd.Variable(target.data.long()).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 32, 32])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_unsup_var.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1000.00 MiB (GPU 0; 10.92 GiB total capacity; 8.18 GiB already allocated; 475.44 MiB free; 27.28 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-174-88e8d15a3ec7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# compute loss for unlabeled input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0moutput_unsup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxhat_unsup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_pn_unsup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_bnmm_unsup\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_unsup_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-0c65b3292019>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    122\u001b[0m                         \u001b[0mxbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_pn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# get the forward results to compute the path normalization later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-0c65b3292019>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1000.00 MiB (GPU 0; 10.92 GiB total capacity; 8.18 GiB already allocated; 475.44 MiB free; 27.28 MiB cached)"
     ]
    }
   ],
   "source": [
    "# compute loss for unlabeled input\n",
    "[output_unsup, xhat_unsup, loss_pn_unsup, loss_bnmm_unsup] = net(input_unsup_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onu/miniconda3/lib/python3.7/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
      "/home/onu/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "multi-target not supported at /opt/conda/conda-bld/pytorch_1550796191843/work/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-c9b6a8d85b4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# compute loss for labeled input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0moutput_sup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxhat_sup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_pn_sup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_bnmm_sup\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sup_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sup_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mloss_xentropy_sup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_sup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sup_var\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mminibatch_sup_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mloss_reconst_sup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL2_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxhat_sup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sup_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0msoftmax_sup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_sup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 904\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1968\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1970\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1788\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1789\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: multi-target not supported at /opt/conda/conda-bld/pytorch_1550796191843/work/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15"
     ]
    }
   ],
   "source": [
    "# main training loop\n",
    "# def train(net, train_loader,labeled_loader, eval_loader, num_epochs, wd):\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0; train_loss_xentropy = 0; train_loss_reconst = 0; train_loss_pn = 0; train_loss_kl = 0; train_loss_bnmm = 0\n",
    "    correct = 0\n",
    "    num_batch_train = 0\n",
    "\n",
    "    # start with adam optimizer but switch sgd optimizer with exponential decay learning rate since epoch 20 \n",
    "    if epoch == 20:\n",
    "        sgd_lr = opt.lr[1]\n",
    "        decay_val = np.exp(np.log(sgd_lr / 0.0001) / (num_epochs - 2))\n",
    "        sgd_lr = sgd_lr * decay_val\n",
    "        trainer = th.optim.SGD(net.parameters(), sgd_lr, weight_decay=wd)\n",
    "\n",
    "    if epoch >= 20:\n",
    "        for param_group in trainer.param_groups:\n",
    "            param_group['lr'] = param_group['lr']/decay_val\n",
    "\n",
    "    for param_group in trainer.param_groups:\n",
    "        learning_rate = param_group['lr']\n",
    "\n",
    "    meters = AverageMeterSet()\n",
    "\n",
    "    # switch to train mode\n",
    "    net.train()\n",
    "\n",
    "#     end = time.time()\n",
    "    for i in range(int(n_iterations)):\n",
    "#         meters.update('data_time', time.time() - end)\n",
    "\n",
    "        unsup_batch = next(iter(train_loader))\n",
    "        sup_batch,target = next(iter(labeled_loader))\n",
    "\n",
    "        # set up unlabeled input and labeled input with the corresponding labels\n",
    "        input_unsup_var = th.autograd.Variable(unsup_batch).to(device)\n",
    "        input_sup_var = th.autograd.Variable(sup_batch).to(device)\n",
    "        target_sup_var = th.autograd.Variable(target.data.long()).to(device)\n",
    "\n",
    "        minibatch_unsup_size = opt.batch_size - opt.labeled_batch_size\n",
    "        minibatch_sup_size = opt.labeled_batch_size\n",
    "\n",
    "        # compute loss for unlabeled input\n",
    "        [output_unsup, xhat_unsup, loss_pn_unsup, loss_bnmm_unsup] = net(input_unsup_var)\n",
    "        loss_reconst_unsup = L2_loss(xhat_unsup, input_unsup_var).mean()\n",
    "        softmax_unsup = F.softmax(output_unsup)\n",
    "        loss_kl_unsup = -th.sum(th.log(10.0*softmax_unsup + 1e-8) * softmax_unsup) / minibatch_unsup_size\n",
    "        loss_unsup = opt.alpha_reconst * loss_reconst_unsup + opt.alpha_kl * loss_kl_unsup + opt.alpha_bnmm * loss_bnmm_unsup + opt.alpha_pn * loss_pn_unsup\n",
    "\n",
    "\n",
    "\n",
    "        # compute loss for labeled input\n",
    "        [output_sup, xhat_sup, loss_pn_sup, loss_bnmm_sup] = net(input_sup_var, target_sup_var)\n",
    "        loss_xentropy_sup = criterion(output_sup, target_sup_var) / minibatch_sup_size\n",
    "        loss_reconst_sup = L2_loss(xhat_sup, input_sup_var).mean()\n",
    "        softmax_sup = F.softmax(output_sup)\n",
    "        loss_kl_sup = -th.sum(th.log(10.0*softmax_sup + 1e-8) * softmax_sup)/ minibatch_sup_size\n",
    "        loss_sup = loss_xentropy_sup + opt.alpha_reconst * loss_reconst_sup + opt.alpha_kl * loss_kl_sup + opt.alpha_bnmm * loss_bnmm_sup + opt.alpha_pn * loss_pn_sup\n",
    "\n",
    "        loss = th.mean(loss_unsup + loss_sup)\n",
    "\n",
    "        # compute the grads and update the parameters\n",
    "        trainer.zero_grad()\n",
    "        loss.backward()\n",
    "        trainer.step()\n",
    "\n",
    "        # accumulate all the losses for visualization\n",
    "        loss_reconst = loss_reconst_unsup + loss_reconst_sup\n",
    "        loss_pn = loss_pn_unsup + loss_pn_sup\n",
    "        loss_xentropy = loss_xentropy_sup\n",
    "        loss_kl = loss_kl_unsup + loss_kl_sup\n",
    "        loss_bnmm = loss_bnmm_unsup + loss_bnmm_sup\n",
    "\n",
    "        train_loss_xentropy += th.mean(loss_xentropy).cpu().detach().numpy()\n",
    "        train_loss_reconst += th.mean(loss_reconst).cpu().detach().numpy()\n",
    "        train_loss_pn += th.mean(loss_pn).cpu().detach().numpy()\n",
    "        train_loss_kl += th.mean(loss_kl).cpu().detach().numpy()\n",
    "        train_loss_bnmm += th.mean(loss_bnmm).cpu().detach().numpy()\n",
    "        train_loss += th.mean(loss).cpu().detach().numpy()\n",
    "        correct += get_acc(output_sup, target_sup_var).cpu().detach().numpy()\n",
    "\n",
    "        num_batch_train += 1\n",
    "        iter_indx += 1\n",
    "\n",
    "    writer.add_scalars('loss', {'train': train_loss / num_batch_train}, epoch)\n",
    "    writer.add_scalars('loss_xentropy', {'train': train_loss_xentropy / num_batch_train}, epoch)\n",
    "    writer.add_scalars('loss_reconst', {'train': train_loss_reconst / num_batch_train}, epoch)\n",
    "    writer.add_scalars('loss_pn', {'train': train_loss_pn / num_batch_train}, epoch)\n",
    "    writer.add_scalars('loss_kl', {'train': train_loss_kl / num_batch_train}, epoch)\n",
    "    writer.add_scalars('loss_bnmm', {'train': train_loss_bnmm / num_batch_train}, epoch)\n",
    "    writer.add_scalars('acc', {'train': correct / num_batch_train}, epoch)\n",
    "\n",
    "    cur_time = datetime.datetime.now()\n",
    "    h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "    m, s = divmod(remainder, 60)\n",
    "    time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "\n",
    "    # Validation\n",
    "    valid_loss = 0; valid_loss_xentropy = 0; valid_loss_reconst = 0; valid_loss_pn = 0; valid_loss_kl = 0; valid_loss_bnmm = 0\n",
    "    valid_correct = 0\n",
    "    num_batch_valid = 0\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    for i, (batch, target) in enumerate(eval_loader):\n",
    "        with th.no_grad():\n",
    "            input_var = th.autograd.Variable(batch).to(device)\n",
    "            target_var = th.autograd.Variable(target).to(device)\n",
    "\n",
    "            minibatch_size = len(target_var)\n",
    "\n",
    "            [output, xhat, loss_pn, loss_bnmm] = net(input_var, target_var)\n",
    "\n",
    "            loss_xentropy = criterion(output, target_var)/minibatch_size\n",
    "            loss_reconst = L2_loss(xhat, input_var).mean()\n",
    "            softmax_val = F.softmax(output)\n",
    "            loss_kl = -th.sum(th.log(10.0*softmax_val + 1e-8) * softmax_val)/minibatch_size\n",
    "            loss = loss_xentropy + opt.alpha_reconst * loss_reconst + opt.alpha_kl * loss_kl + opt.alpha_bnmm * loss_bnmm + opt.alpha_pn * loss_pn\n",
    "\n",
    "            valid_loss_xentropy += th.mean(loss_xentropy).cpu().detach().numpy()\n",
    "            valid_loss_reconst += th.mean(loss_reconst).cpu().detach().numpy()\n",
    "            valid_loss_pn += th.mean(loss_pn).cpu().detach().numpy()\n",
    "            valid_loss_kl += th.mean(loss_kl).cpu().detach().numpy()\n",
    "            valid_loss_bnmm += th.mean(loss_bnmm).cpu().detach().numpy()\n",
    "            valid_loss += th.mean(loss).cpu().detach().numpy()\n",
    "            valid_correct += get_acc(output, target_var).cpu().detach().numpy()\n",
    "\n",
    "            num_batch_valid += 1\n",
    "\n",
    "    valid_acc = valid_correct / num_batch_valid\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "#         th.save(net.state_dict(), '%s/%s_best.pth'%(opt.model_dir, opt.exp_name))\n",
    "#     writer.add_scalars('loss', {'valid': valid_loss / num_batch_valid}, epoch)\n",
    "#     writer.add_scalars('loss_xentropy', {'valid': valid_loss_xentropy / num_batch_valid}, epoch)\n",
    "#     writer.add_scalars('loss_reconst', {'valid': valid_loss_reconst / num_batch_valid}, epoch)\n",
    "#     writer.add_scalars('loss_pn', {'valid': valid_loss_pn / num_batch_valid}, epoch)\n",
    "#     writer.add_scalars('loss_kl', {'valid': valid_loss_kl / num_batch_valid}, epoch)\n",
    "#     writer.add_scalars('loss_bnmm', {'valid': valid_loss_bnmm / num_batch_valid}, epoch)\n",
    "#     writer.add_scalars('acc', {'valid': valid_acc}, epoch)\n",
    "    epoch_str = (\"Epoch %d. Train Loss: %f, Train Xent: %f, Train Reconst: %f, Train Pn: %f, Train acc %f, Valid Loss: %f, Valid acc %f, Best valid acc %f, \"\n",
    "                 % (epoch, train_loss / num_batch_train, train_loss_xentropy / num_batch_train, train_loss_reconst / num_batch_train, train_loss_pn / num_batch_train,\n",
    "                    correct / num_batch_train, valid_loss / num_batch_valid, valid_acc, best_valid_acc))\n",
    "    if not epoch % 20:\n",
    "        th.save(net.state_dict(), '%s/%s_epoch_%i.pth'%(opt.model_dir, opt.exp_name, epoch))\n",
    "\n",
    "    prev_time = cur_time\n",
    "#     logging.info(epoch_str + time_str + ', lr ' + str(learning_rate))\n",
    "    print(epoch_str)   \n",
    "#     return best_valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6],\n",
       "        [ 0],\n",
       "        [16]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.data.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        th.nn.init.xavier_uniform(m.weight)\n",
    "        # m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Bias') != -1:\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "#     acc = train(model, train_loader, eval_loader, opt.num_epochs, opt.weight_decay)\n",
    "#     logging.info('Validation Accuracy - Run %i = %f'%(i, acc))\n",
    "#     valid_acc += acc\n",
    "\n",
    "#     logging.info('Validation Accuracy = %f'%(valid_acc/num_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "\n",
    "cfg = {\n",
    "    'AllConv13': [128, 128, 128, 'M', 256, 256, 256, 'M', 512, 256, 128, 'A'],\n",
    "}\n",
    "\n",
    "#################### Some utils class ####################\n",
    "class Reshape(nn.Module):\n",
    "    \"\"\"\n",
    "    Flatten the output of the convolutional layer\n",
    "    Parameters\n",
    "    ----------\n",
    "    Input shape: (N, C * W * H)\n",
    "    Output shape: (N, C, W, H)\n",
    "    \"\"\"\n",
    "    def __init__(self, shape, **kwargs):\n",
    "        super(Reshape, self).__init__(**kwargs)\n",
    "        self._shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.reshape(x.size()[0], self._shape[0], self._shape[1], self._shape[2])\n",
    "    \n",
    "class BiasAdder(nn.Module):\n",
    "    \"\"\"\n",
    "    Add a bias into the input\n",
    "    \"\"\"\n",
    "    def __init__ (self, channels, **kwargs):\n",
    "        super(BiasAdder, self).__init__(**kwargs)\n",
    "        self.bias = nn.Parameter(th.Tensor(1,channels,1,1))\n",
    "        self.bias.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.bias\n",
    "    \n",
    "class Flatten(nn.Module):\n",
    "    \"\"\"\n",
    "    Flatten 4D tensor into 2D tensor\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "    \n",
    "class Upaverage(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsample to reverse the avg pooling layer\n",
    "    \"\"\"\n",
    "    def __init__(self, scale_factor, **kwargs):\n",
    "        super(Upaverage, self).__init__(**kwargs)\n",
    "        self.scale_factor = scale_factor\n",
    "        self.upsample_layer = nn.Upsample(scale_factor=self.scale_factor, mode='nearest')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.upsample_layer(x) * (1./self.scale_factor)**2\n",
    "    \n",
    "def make_one_hot(labels, C=2):\n",
    "    \"\"\"\n",
    "    Converts an integer label torch.autograd.Variable to a one-hot Variable.\n",
    "    \"\"\"\n",
    "    target = th.eye(C)[labels.data]\n",
    "    target = target.to(labels.get_device())      \n",
    "    return target\n",
    "\n",
    "#################### Main NRM class ####################\n",
    "class NRM(nn.Module):\n",
    "    def __init__(self, net_name, batch_size, num_class, use_bias=False, use_bn=False, do_topdown=False, do_pn=False, do_bnmm=False):\n",
    "        super(NRM, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.do_topdown = do_topdown\n",
    "        self.do_pn = do_pn\n",
    "        self.do_bnmm = do_bnmm\n",
    "        self.use_bn = use_bn\n",
    "        self.use_bias = use_bias\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # create:\n",
    "        # feature extractor in the forward cnn step: self.features\n",
    "        # corresponding layer inm the top-down reconstruction nrm step: layers_nrm\n",
    "        # instance norm used in the top-down reconstruction nrm step: insnorms_nrm\n",
    "        # instance norm used in the forward cnn step: insnorms_cnn\n",
    "        self.features, layers_nrm, insnorms_nrm, insnorms_cnn = self._make_layers(cfg[net_name], use_bias, use_bn, self.do_topdown)\n",
    "        \n",
    "        # create the classifer in the forward cnn step\n",
    "        conv_layer = nn.Conv2d(in_channels=cfg[net_name][-2], out_channels=self.num_class, kernel_size=(1,1), bias=True)\n",
    "        flatten_layer = Flatten()\n",
    "        self.classifier = nn.Sequential(OrderedDict([('conv',conv_layer), ('flatten', flatten_layer)]))\n",
    "        \n",
    "        # create the nrm\n",
    "        if self.do_topdown:\n",
    "            # add layers corresponding to the classifer in the forward step\n",
    "            convtd_layer = nn.ConvTranspose2d(out_channels=cfg[net_name][-2], in_channels=self.num_class, kernel_size=(1,1), stride=(1, 1), bias=False)\n",
    "            convtd_layer.weight.data = conv_layer.weight.data\n",
    "            layers_nrm += [('convtd',convtd_layer), ('reshape', Reshape(shape=(self.num_class, 1, 1)))]\n",
    "            \n",
    "            self.nrm = nn.Sequential(OrderedDict(layers_nrm[::-1]))\n",
    "            \n",
    "            # if use path normalization, then also use instance normalization\n",
    "            if self.do_pn:\n",
    "                self.insnorms_nrm = nn.Sequential(OrderedDict(insnorms_nrm[::-1]))\n",
    "                self.insnorms_cnn = nn.Sequential(OrderedDict(insnorms_cnn))\n",
    "\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        ahat = []; that = []; bcnn = []; apn = []; meancnn = []; varcnn = []\n",
    "        xbias = th.zeros([1, x.shape[1], x.shape[2], x.shape[3]], device=x.get_device()) if self.do_pn else []\n",
    "        insnormcnn_indx = 0\n",
    "        \n",
    "        # if do top-down reconstruction, we need to keep track of relu state, maxpool state,\n",
    "        # mean and var of the activations, and the bias terms in the forward cnn step\n",
    "        if self.do_topdown: \n",
    "            for name, layer in self.features.named_children():\n",
    "                if name.find('pool') != -1 and not name.find('average') != -1: # keep track of the maxpool state\n",
    "                    F.interpolate(layer(x), scale_factor=2, mode='nearest')\n",
    "                    that.append(th.gt(x-F.interpolate(layer(x), scale_factor=2, mode='nearest'),0))\n",
    "                    x = layer(x)\n",
    "                    if self.do_pn:\n",
    "                        xbias = layer(xbias)\n",
    "                else:\n",
    "                    x = layer(x)\n",
    "                    \n",
    "                    if self.do_pn: # get the forward results to compute the path normalization later\n",
    "                        if name.find('batchnorm') != -1:\n",
    "                            xbias = self.insnorms_cnn[insnormcnn_indx](xbias)\n",
    "                            insnormcnn_indx += 1\n",
    "                        else:\n",
    "                            xbias = layer(xbias)\n",
    "                    if name.find('relu') != -1: # keep track of the relu state\n",
    "                        ahat.append(th.gt(x,0) + th.le(x,0)*0.1)\n",
    "                        if self.do_pn:\n",
    "                            apn.append(th.gt(xbias,0) + th.le(xbias,0)*0.1)\n",
    "                    \n",
    "                    if self.use_bn:\n",
    "                        if name.find('conv') != -1: # keep track of the mean and var of the activations\n",
    "                            meancnn.append(th.mean(x, dim=(0,2,3), keepdim=True))\n",
    "                            varcnn.append(th.mean((x - th.mean(x, dim=(0,2,3), keepdim=True))**2, dim=(0,2,3), keepdim=True))\n",
    "                        if self.use_bias: # keep track of the bias terms when adding bias\n",
    "                            if name.find('bias') != -1: \n",
    "                                bcnn.append(layer.bias)\n",
    "                        else: # otherwise, keep track of the bias terms inside the batch norm\n",
    "                            if name.find('batchnorm') != -1:\n",
    "                                bcnn.append(layer.bias)\n",
    "                    else:\n",
    "                        if self.use_bias:\n",
    "                            if name.find('conv') != -1:\n",
    "                                bcnn.append(layer.bias)\n",
    "            \n",
    "            # reverse the order of the parameters/variables that we keep track to use in the top-down reconstruction nrm step since nrm is the reverse of cnn\n",
    "            ahat = ahat[::-1]\n",
    "            that = that[::-1]\n",
    "            bcnn = bcnn[::-1]\n",
    "            apn = apn[::-1]\n",
    "            meancnn = meancnn[::-1]\n",
    "            varcnn = varcnn[::-1]\n",
    "        else:\n",
    "            x =  self.features(x)\n",
    "        \n",
    "        # send the features into the classifier\n",
    "        z = self.classifier(x)\n",
    "        \n",
    "        # do reconstruction via nrm\n",
    "        # xhat: the reconstruction image\n",
    "        # loss_pn: path normalization loss\n",
    "        # loss_bnmm: batch norm moment matching loss\n",
    "        if self.do_topdown:\n",
    "            xhat, _, loss_pn, loss_bnmm = self.topdown(self.nrm, make_one_hot(y, self.num_class), ahat, that, bcnn, th.ones([1, z.size()[1]], device=z.get_device()), apn, meancnn, varcnn) if y is not None else self.topdown(self.nrm, make_one_hot(th.argmax(z.detach(), dim=1), self.num_class), ahat, that, bcnn, th.ones([1, z.size()[1]], device=z.get_device()), apn, meancnn, varcnn)\n",
    "        else:\n",
    "            xhat = None\n",
    "            loss_pn = None\n",
    "            loss_bnmm = None\n",
    "\n",
    "\n",
    "        return [z, xhat, loss_pn, loss_bnmm]\n",
    "\n",
    "    def _make_layers(self, cfg, use_bias, use_bn, do_topdown):\n",
    "        layers = []\n",
    "        layers_nrm = []\n",
    "        insnorms_nrm = []\n",
    "        insnorms_cnn = []\n",
    "        in_channels = 3\n",
    "\n",
    "        for i, x in enumerate(cfg):\n",
    "            if x == 'M': # if max pooling layer, then add max pooling and dropout into the cnn. Add upsample layers, dropout, batchnorm, and instance norm - for path normaliztion - into the nrm.\n",
    "                layers += [('pool%i'%i, nn.MaxPool2d(2, stride=2)), ('dropout%i'%i, nn.Dropout(0.5))]\n",
    "                if do_topdown:\n",
    "                    if use_bn:\n",
    "                        layers_nrm += [('upsample%i'%i, nn.Upsample(scale_factor=2, mode='nearest')), ('dropout%i'%i, nn.Dropout(0.5)), ('batchnorm%i'%i, nn.BatchNorm2d(cfg[i-1]))]\n",
    "                        insnorms_nrm += [('instancenormtd%i'%i, nn.InstanceNorm2d(cfg[i-1], affine=True))]\n",
    "                    else:\n",
    "                        layers_nrm += [('upsample%i'%i, nn.Upsample(scale_factor=2, mode='nearest')), ('dropout%i'%i, nn.Dropout(0.5))]\n",
    "                        \n",
    "            elif x == 'A': # if avg pooling layer, then add average pooling layer into the cnn. Add up average layers, batchnorm and instance norm - for path normaliztion - into the nrm.\n",
    "                layers += [('average%i'%i, nn.AvgPool2d(6, stride=1))]\n",
    "                if do_topdown:\n",
    "                    if use_bn:\n",
    "                        layers_nrm += [('upaverage%i'%i, Upaverage(scale_factor=6)), ('batchnorm%i'%i, nn.BatchNorm2d(cfg[i-1]))]\n",
    "                        insnorms_nrm += [('instancenormtd%i'%i, nn.InstanceNorm2d(cfg[i-1], affine=True))]\n",
    "                    else:\n",
    "                        layers_nrm += [('upaverage%i'%i, Upaverage(scale_factor=6))]\n",
    "                        \n",
    "            else: # add other layers into the cnn and the nrm\n",
    "                padding_cnn = (0,0) if x == 512 else (1,1)\n",
    "                padding_nrm = (0,0) if x == 512 else (1,1)\n",
    "                if use_bn:\n",
    "                    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=x, kernel_size=(3,3), padding=padding_cnn, bias=False)\n",
    "                    if use_bias:\n",
    "                        layers += [('conv%i'%i, conv_layer),\n",
    "                                   ('batchnorm%i'%i, nn.BatchNorm2d(x)),\n",
    "                                   ('bias%i'%i,BiasAdder(channels=x)),\n",
    "                                   ('relu%i'%i,nn.LeakyReLU(0.1))]\n",
    "                    else:\n",
    "                        layers += [('conv%i'%i, conv_layer),\n",
    "                                   ('batchnorm%i'%i, nn.BatchNorm2d(x)),\n",
    "                                   ('relu%i'%i, nn.LeakyReLU(0.1))]\n",
    "                    \n",
    "                    insnorms_cnn += [('instancenormcnn%i'%i, nn.InstanceNorm2d(x, affine=True))]\n",
    "                    if do_topdown:\n",
    "                        if (cfg[i-1] == 'M' or cfg[i-1] == 'A') and not i == 0:\n",
    "                            layers_nrm += [('convtd%i'%i, nn.ConvTranspose2d(out_channels=in_channels, in_channels=x, kernel_size=3, stride=(1, 1),\n",
    "                                                              padding=padding_nrm, bias=False))]\n",
    "                            layers_nrm[-1][-1].weight.data = conv_layer.weight.data\n",
    "                        else:\n",
    "                            layers_nrm += [('batchnormtd%i'%i, nn.BatchNorm2d(in_channels)), ('convtd%i'%i, nn.ConvTranspose2d(out_channels=in_channels, in_channels=x, kernel_size=3, stride=(1, 1), padding=padding_nrm, bias=False))]\n",
    "                            layers_nrm[-1][-1].weight.data = conv_layer.weight.data\n",
    "                            insnorms_nrm += [('instancenormtd%i'%i, nn.InstanceNorm2d(in_channels, affine=True))]\n",
    "                    \n",
    "                elif use_bias:\n",
    "                    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=x, kernel_size=(3,3), padding=padding_cnn, use_bias=True)\n",
    "                    layers += [('conv%i'%i, conv_layer), ('relu%i'%i, nn.LeakyReLU(0.1))]\n",
    "                    if do_topdown:\n",
    "                        layers_nrm += [('convtd%i'%i, nn.ConvTranspose2d(out_channels=in_channels, in_channels=x, kernel_size=3, stride=(1, 1),\n",
    "                                                          padding=padding_nrm, bias=False))]\n",
    "                        layers_nrm[-1][-1].weight.data = conv_layer.weight.data\n",
    "                    \n",
    "                else:\n",
    "                    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=x, kernel_size=(3,3), padding=padding_cnn, bias=False)\n",
    "                    layers += [('conv%i'%i, conv_layer), ('relu%i'%i, nn.LeakyReLU(0.1))]\n",
    "                    if do_topdown:\n",
    "                        layers_nrm += [('convtd%i'%i, nn.ConvTranspose2d(out_channels=in_channels, in_channels=x, kernel_size=3, stride=(1,1),\n",
    "                                                          padding=padding_nrm, bias=False))]\n",
    "                        layers_nrm[-1][-1].weight.data = conv_layer.weight.data\n",
    "                        \n",
    "                in_channels = x\n",
    "\n",
    "        model = nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "        return model, layers_nrm, insnorms_nrm, insnorms_cnn\n",
    "\n",
    "    def topdown(self, net, xhat, ahat, that, bcnn, xpn, apn, meancnn, varcnn):\n",
    "        mu = xhat\n",
    "        mupn = xpn\n",
    "        loss_pn = th.zeros([self.batch_size,], device=mu.get_device())\n",
    "        loss_bnmm = th.zeros([self.batch_size,], device=mu.get_device())\n",
    "\n",
    "        ahat_indx = 0; that_indx = 0; meanvar_indx = 0; insnormtd_indx = 0\n",
    "        prev_name = ''\n",
    "        \n",
    "        for i, (name, layer) in enumerate(net.named_children()):\n",
    "            if name.find('conv') != -1 and i > 1: \n",
    "                mu = mu * ahat[ahat_indx].type(th.FloatTensor).to(mu.get_device()) # mask the intermediate rendered images by the relu states in the forward step\n",
    "                \n",
    "                if self.do_pn: # compute the path normalization loss\n",
    "                    mupn = mupn * apn[ahat_indx].type(th.FloatTensor).to(mu.get_device())\n",
    "                    mu_b = bcnn[ahat_indx].data.reshape((1, -1, 1, 1)) * mu\n",
    "                    mupn_b = bcnn[ahat_indx].data.reshape((1, -1, 1, 1)) * mupn\n",
    "                    \n",
    "                    loss_pn_layer = th.mean(th.abs(mu_b - mupn_b), dim=(1,2,3))\n",
    "                    loss_pn = loss_pn + loss_pn_layer\n",
    "\n",
    "                ahat_indx += 1\n",
    "\n",
    "            if prev_name.find('upsamplelayer') != -1 and not prev_name.find('avg') != -1:\n",
    "                mu = mu * that[that_indx].type(th.FloatTensor).to(mu.get_device()) # mask the intermediate rendered images by the maxpool states in the forward step\n",
    "                if self.do_pn:\n",
    "                    mupn = mupn * that[that_indx].type(th.FloatTensor).to(mu.get_device())\n",
    "                that_indx += 1\n",
    "          \n",
    "            # compute the next intermediate rendered images\n",
    "            mu = layer(mu)\n",
    "            \n",
    "            # compute the next intermediate rendered results for computing the path normalization loss in the next layer\n",
    "            if (name.find('batchnorm') != -1) and (i < len(net) - 1):\n",
    "                if self.do_pn:\n",
    "                    mupn = self.insnorms_nrm[insnormtd_indx](mupn)\n",
    "                    insnormtd_indx += 1\n",
    "            else:\n",
    "                if self.do_pn:\n",
    "                    mupn = layer(mupn)\n",
    "            \n",
    "            if (name.find('conv') != -1) and (i != (len(net)-2)):\n",
    "                if self.do_bnmm and self.use_bn:\n",
    "                    # compute the KL distance between two Gaussians - the intermediate rendered images and the mean/var from the forward step\n",
    "                    loss_bnmm = loss_bnmm + 0.5*th.mean(((th.mean(mu, dim=(0,2,3)) - meancnn[meanvar_indx])**2)/varcnn[meanvar_indx]) + 0.5*th.mean(th.mean((mu - th.mean(mu, dim=(0,2,3), keepdim=True))**2, dim=(0,2,3))/varcnn[meanvar_indx]) - 0.5*th.mean(th.log(th.mean((mu - th.mean(mu, dim=(0,2,3), keepdim=True))**2, dim=(0,2,3)) + 1e-8) - th.log(varcnn[meanvar_indx])) - 0.5\n",
    "                    meanvar_indx += 1\n",
    "                    \n",
    "            prev_name = name\n",
    "            \n",
    "        return mu, mupn, loss_pn, loss_bnmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
