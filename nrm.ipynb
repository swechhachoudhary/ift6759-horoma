{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import logging\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "import torchvision.datasets\n",
    "\n",
    "from utils import data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import shutil\n",
    "import _pickle as cPickle\n",
    "from sklearn import preprocessing\n",
    "import subprocess\n",
    "\n",
    "from IPython.core.debugger import Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.seed_val = 0 # random seed val\n",
    "        self.num_train_sup = 228 # number of labeled train samples\n",
    "        self.batch_size = 6 # batch size\n",
    "        self.labeled_batch_size = 3 # number of labeled samples in a batch\n",
    "        self.device = 0 # gpu id\n",
    "        \n",
    "        self.lr = [0.001, 0.15] # learning rate for adam and then and then initial learning rate for sgd respectively\n",
    "        self.num_epochs = 500 # number of training epochs\n",
    "        self.weight_decay = 5e-4 # weight decay\n",
    "        \n",
    "        self.mount_point = '/home/onu/Desktop/' # change this to your mount_point\n",
    "        self.datadir = 'data-local/images/cifar/cifar10/by-image' # dataset directory\n",
    "        self.labels = 'data-local/labels/cifar10/4000_balanced_labels/00.txt' # label directory\n",
    "        self.log_dir = os.path.join(self.mount_point,'logs') # log directory\n",
    "        self.model_dir = os.path.join(self.mount_point,'models') # log \n",
    "        self.exp_name = 'cifar10_nl_%i_allconv13_seed_%i'%(self.num_train_sup, self.seed_val) # name of experiments\n",
    "        \n",
    "        self.train_subdir = 'train+val'\n",
    "        self.eval_subdir = 'test'\n",
    "        self.num_classes = 17\n",
    "        self.workers = 4\n",
    "        \n",
    "        self.alpha_reconst = 0.5 # weight for reconstruction loss\n",
    "        self.alpha_pn = 1.0 # weight for path normalization loss\n",
    "        self.alpha_kl = 0.5 # weight for kl loss\n",
    "        self.alpha_bnmm = 0.5 # weight for moment matching loss when doing batchnorm\n",
    "        \n",
    "        self.use_bias = True # add bias after batchnorm\n",
    "        self.use_bn = True # use batch norm\n",
    "        self.do_topdown = True # do topdown\n",
    "        self.do_pn = True # do path normalization\n",
    "        self.do_bnmm = True # do moment matching for batchnorm\n",
    "\n",
    "opt = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "th.cuda.set_device(opt.device)\n",
    "device = th.device(\"cuda:%i\"%opt.device if (opt.device >= 0) else \"cpu\")\n",
    "\n",
    "def gpu_device(device=0):\n",
    "    try:\n",
    "        _ = th.tensor([1, 2, 3], device=th.device('cuda', device))\n",
    "    except ValueError:\n",
    "        return None\n",
    "    return th.device('cuda', device)\n",
    "\n",
    "assert gpu_device(opt.device), 'No GPU device found!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make required folders\n",
    "if not os.path.exists(opt.log_dir):\n",
    "    os.makedirs(opt.log_dir)\n",
    "log_dir = os.path.join(opt.log_dir, opt.exp_name)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "if not os.path.exists(opt.model_dir):\n",
    "    os.makedirs(opt.model_dir)\n",
    "if not os.path.exists(os.path.join(opt.mount_point,'datasets')):\n",
    "    os.makedirs(os.path.join(opt.mount_point,'datasets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-09 17:34:52,992 - <__main__.Options object at 0x7fc910b27e80>\n"
     ]
    }
   ],
   "source": [
    "# set logging option\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "console = logging.StreamHandler()\n",
    "console.setFormatter(formatter)\n",
    "logger.addHandler(console)\n",
    "\n",
    "hdlr = logging.FileHandler(os.path.join(opt.log_dir, '{}.log'.format(opt.exp_name)))\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logging.info(opt)\n",
    "\n",
    "writer = SummaryWriter(os.path.join(opt.log_dir, opt.exp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onu/miniconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/onu/miniconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# set losses\n",
    "NO_LABEL = -1\n",
    "criterion = nn.CrossEntropyLoss(size_average=False, ignore_index=NO_LABEL).cuda()\n",
    "L2_loss = nn.MSELoss(size_average=False, reduce=False, reduction='mean').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some util functions\n",
    "def get_acc(output, label):\n",
    "    pred = th.argmax(output, dim=1, keepdim=False)\n",
    "    correct = th.mean((pred == label).type(th.FloatTensor))\n",
    "    return correct\n",
    "\n",
    "class AverageMeterSet:\n",
    "    def __init__(self):\n",
    "        self.meters = {}\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.meters[key]\n",
    "\n",
    "    def update(self, name, value, n=1):\n",
    "        if not name in self.meters:\n",
    "            self.meters[name] = AverageMeter()\n",
    "        self.meters[name].update(value, n)\n",
    "\n",
    "    def reset(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.reset()\n",
    "\n",
    "    def values(self, postfix=''):\n",
    "        return {name + postfix: meter.val for name, meter in self.meters.items()}\n",
    "\n",
    "    def averages(self, postfix='/avg'):\n",
    "        return {name + postfix: meter.avg for name, meter in self.meters.items()}\n",
    "\n",
    "    def sums(self, postfix='/sum'):\n",
    "        return {name + postfix: meter.sum for name, meter in self.meters.items()}\n",
    "\n",
    "    def counts(self, postfix='/count'):\n",
    "        return {name + postfix: meter.count for name, meter in self.meters.items()}\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __format__(self, format):\n",
    "        return \"{self.val:{format}} ({self.avg:{format}})\".format(self=self, format=format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set:  (152228, 3, 32, 32)\n",
      "Shape of validation set:  (228, 3, 32, 32)\n",
      "Shape of validation set:  (252, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "from data.dataset import HoromaDataset\n",
    "\n",
    "from comet_ml import OfflineExperiment\n",
    "import json\n",
    "import argparse\n",
    "from models import *\n",
    "from models.clustering import *\n",
    "from utils.ali_utils import *\n",
    "from utils.utils import *\n",
    "from utils.utils import load_datasets\n",
    "from utils.constants import Constants\n",
    "from data.dataset import HoromaDataset\n",
    "import torch\n",
    "path_to_model = None\n",
    "config_key = 'HALI'\n",
    "config = 'HALI'\n",
    "\n",
    "with open(Constants.CONFIG_PATH, 'r') as f:\n",
    "    configuration = json.load(f)[config_key]\n",
    "\n",
    "# Parse configuration file\n",
    "clustering_model = configuration['cluster_model']\n",
    "encoding_model = configuration['enc_model']\n",
    "batch_size = configuration['batch_size']\n",
    "seed = configuration['seed']\n",
    "n_epochs = configuration['n_epochs']\n",
    "train_subset = configuration['train_subset']\n",
    "train_split = configuration['train_split']\n",
    "valid_split = configuration['valid_split']\n",
    "train_labeled_split = configuration['train_labeled_split']\n",
    "encode = configuration['encode']\n",
    "cluster = configuration['cluster']\n",
    "flattened = False  # Default\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set all seeds for full reproducibility\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "datapath = Constants.DATAPATH\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "train = HoromaDataset(datapath, split=train_split, subset=train_subset,\n",
    "                      flattened=flattened)\n",
    "labeled = HoromaDataset(datapath, split=train_labeled_split, subset=train_subset,\n",
    "                        flattened=flattened)\n",
    "valid_data = HoromaDataset(\n",
    "    datapath, split=valid_split, subset=train_subset, flattened=flattened)\n",
    "\n",
    "train_label_indices = labeled.targets\n",
    "valid_indices = valid_data.targets\n",
    "\n",
    "print(\"Shape of training set: \", train.data.shape)\n",
    "print(\"Shape of validation set: \", labeled.data.shape)\n",
    "print(\"Shape of validation set: \", valid_data.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=opt.batch_size, shuffle=True)\n",
    "labeled_loader = DataLoader(labeled, batch_size=opt.labeled_batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(valid_data, batch_size=opt.labeled_batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.0\n"
     ]
    }
   ],
   "source": [
    "n_iterations = np.floor(labeled.data.shape[0]/opt.labeled_batch_size)\n",
    "print(n_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onu/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NRM(\n",
       "  (features): Sequential(\n",
       "    (conv0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (batchnorm0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bias0): BiasAdder()\n",
       "    (relu0): LeakyReLU(negative_slope=0.1)\n",
       "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (batchnorm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bias1): BiasAdder()\n",
       "    (relu1): LeakyReLU(negative_slope=0.1)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bias2): BiasAdder()\n",
       "    (relu2): LeakyReLU(negative_slope=0.1)\n",
       "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (dropout3): Dropout(p=0.5)\n",
       "    (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (batchnorm4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bias4): BiasAdder()\n",
       "    (relu4): LeakyReLU(negative_slope=0.1)\n",
       "    (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (batchnorm5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bias5): BiasAdder()\n",
       "    (relu5): LeakyReLU(negative_slope=0.1)\n",
       "    (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (batchnorm6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bias6): BiasAdder()\n",
       "    (relu6): LeakyReLU(negative_slope=0.1)\n",
       "    (pool7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (dropout7): Dropout(p=0.5)\n",
       "    (conv8): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (batchnorm8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bias8): BiasAdder()\n",
       "    (relu8): LeakyReLU(negative_slope=0.1)\n",
       "    (conv9): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (batchnorm9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bias9): BiasAdder()\n",
       "    (relu9): LeakyReLU(negative_slope=0.1)\n",
       "    (conv10): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (batchnorm10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bias10): BiasAdder()\n",
       "    (relu10): LeakyReLU(negative_slope=0.1)\n",
       "    (average11): AvgPool2d(kernel_size=6, stride=1, padding=0)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (conv): Conv2d(128, 17, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (flatten): Flatten()\n",
       "  )\n",
       "  (nrm): Sequential(\n",
       "    (reshape): Reshape()\n",
       "    (convtd): ConvTranspose2d(17, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (batchnorm11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (upaverage11): Upaverage(\n",
       "      (upsample_layer): Upsample(scale_factor=6, mode=nearest)\n",
       "    )\n",
       "    (convtd10): ConvTranspose2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (batchnormtd10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (convtd9): ConvTranspose2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (batchnormtd9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (convtd8): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (batchnorm7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout7): Dropout(p=0.5)\n",
       "    (upsample7): Upsample(scale_factor=2, mode=nearest)\n",
       "    (convtd6): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (batchnormtd6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (convtd5): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (batchnormtd5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (convtd4): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (batchnorm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout3): Dropout(p=0.5)\n",
       "    (upsample3): Upsample(scale_factor=2, mode=nearest)\n",
       "    (convtd2): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (batchnormtd2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (convtd1): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (batchnormtd1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (convtd0): ConvTranspose2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (batchnormtd0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (insnorms_nrm): Sequential(\n",
       "    (instancenormtd11): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (instancenormtd10): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (instancenormtd9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (instancenormtd7): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (instancenormtd6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (instancenormtd5): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (instancenormtd3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (instancenormtd2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (instancenormtd1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (instancenormtd0): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  )\n",
       "  (insnorms_cnn): Sequential(\n",
       "    (instancenormcnn0): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (instancenormcnn1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (instancenormcnn2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (instancenormcnn4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (instancenormcnn5): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (instancenormcnn6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (instancenormcnn8): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (instancenormcnn9): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (instancenormcnn10): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NRM('AllConv13', batch_size=opt.batch_size//2, num_class=17, use_bias=opt.use_bias, use_bn=opt.use_bn, do_topdown=opt.do_topdown, do_pn=opt.do_pn, do_bnmm=opt.do_bnmm).to(device)\n",
    "net.apply(weights_init)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = th.optim.Adam(net.parameters(), opt.lr[0], weight_decay=opt.weight_decay)\n",
    "\n",
    "prev_time = datetime.datetime.now()\n",
    "best_valid_acc = 0\n",
    "iter_indx = 0\n",
    "\n",
    "epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsup_batch = next(iter(train_loader))\n",
    "sup_batch,target = next(iter(labeled_loader))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsup_batch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up unlabeled input and labeled input with the corresponding labels\n",
    "input_unsup_var = th.autograd.Variable(unsup_batch[0:(opt.batch_size - opt.labeled_batch_size)]).to(device)\n",
    "input_sup_var = th.autograd.Variable(sup_batch).to(device)\n",
    "target_sup_var = th.autograd.Variable(target.data.long()).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_unsup_size = opt.batch_size\n",
    "minibatch_sup_size = opt.labeled_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute loss for unlabeled input\n",
    "[output_unsup, xhat_unsup, loss_pn_unsup, loss_bnmm_unsup] = net(input_unsup_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_reconst_unsup = L2_loss(xhat_unsup, input_unsup_var).mean()\n",
    "softmax_unsup = F.softmax(output_unsup)\n",
    "loss_kl_unsup = -th.sum(th.log(10.0*softmax_unsup + 1e-8) * softmax_unsup) / minibatch_unsup_size\n",
    "loss_unsup = opt.alpha_reconst * loss_reconst_unsup + opt.alpha_kl * loss_kl_unsup + opt.alpha_bnmm * loss_bnmm_unsup + opt.alpha_pn * loss_pn_unsup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute loss for labeled input\n",
    "[output_sup, xhat_sup, loss_pn_sup, loss_bnmm_sup] = net(input_sup_var, target_sup_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_xentropy_sup = criterion(output_sup, target_sup_var.squeeze_()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_xentropy_sup = criterion(output_sup, target_sup_var.squeeze_()) / minibatch_sup_size\n",
    "loss_reconst_sup = L2_loss(xhat_sup, input_sup_var).mean()\n",
    "softmax_sup = F.softmax(output_sup)\n",
    "loss_kl_sup = -th.sum(th.log(10.0*softmax_sup + 1e-8) * softmax_sup)/ minibatch_sup_size\n",
    "loss_sup = loss_xentropy_sup + opt.alpha_reconst * loss_reconst_sup + opt.alpha_kl * loss_kl_sup + opt.alpha_bnmm * loss_bnmm_sup + opt.alpha_pn * loss_pn_sup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onu/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:49: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/onu/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/onu/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:120: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30. Train Loss: 2.956289, Train Xent: 2.608154, Train Reconst: 0.016058, Train Pn: 0.071542, Train acc 0.298246, Valid Loss: 5.133420, Valid acc 0.329365, Best f1 acc 0.279762,f1 0.279762, acc 0.329365 \n",
      "Epoch 31. Train Loss: 5.218093, Train Xent: 2.674425, Train Reconst: 0.018138, Train Pn: 0.069681, Train acc 0.302632, Valid Loss: 3.653008, Valid acc 0.301587, Best f1 acc 0.279762,f1 0.237302, acc 0.301587 \n",
      "Epoch 32. Train Loss: 3.397070, Train Xent: 2.719332, Train Reconst: 0.016569, Train Pn: 0.066221, Train acc 0.263158, Valid Loss: 4.131181, Valid acc 0.305556, Best f1 acc 0.280423,f1 0.280423, acc 0.305556 \n",
      "Epoch 33. Train Loss: 3.622095, Train Xent: 2.650633, Train Reconst: 0.016285, Train Pn: 0.067332, Train acc 0.285088, Valid Loss: 6.664385, Valid acc 0.281746, Best f1 acc 0.280423,f1 0.212434, acc 0.281746 \n",
      "Epoch 34. Train Loss: 3.856143, Train Xent: 2.632510, Train Reconst: 0.014388, Train Pn: 0.064078, Train acc 0.293860, Valid Loss: 4.133035, Valid acc 0.301587, Best f1 acc 0.280423,f1 0.242460, acc 0.301587 \n",
      "Epoch 35. Train Loss: 3.478221, Train Xent: 2.646100, Train Reconst: 0.016239, Train Pn: 0.066096, Train acc 0.258772, Valid Loss: 5.028127, Valid acc 0.301587, Best f1 acc 0.280423,f1 0.270503, acc 0.301587 \n",
      "Epoch 36. Train Loss: 3.111748, Train Xent: 2.491881, Train Reconst: 0.015845, Train Pn: 0.064115, Train acc 0.333333, Valid Loss: 5.690549, Valid acc 0.289683, Best f1 acc 0.280423,f1 0.213889, acc 0.289683 \n",
      "Epoch 37. Train Loss: 6.202110, Train Xent: 2.518180, Train Reconst: 0.019525, Train Pn: 0.062335, Train acc 0.355263, Valid Loss: 6.586893, Valid acc 0.313492, Best f1 acc 0.284524,f1 0.284524, acc 0.313492 \n",
      "Epoch 38. Train Loss: 2.235561, Train Xent: 2.344784, Train Reconst: 0.016407, Train Pn: 0.064406, Train acc 0.377193, Valid Loss: 8.469077, Valid acc 0.337302, Best f1 acc 0.284524,f1 0.284259, acc 0.337302 \n",
      "Epoch 39. Train Loss: 2.796949, Train Xent: 2.234379, Train Reconst: 0.016827, Train Pn: 0.067901, Train acc 0.372807, Valid Loss: 17.170518, Valid acc 0.309524, Best f1 acc 0.284524,f1 0.246958, acc 0.309524 \n",
      "Epoch 40. Train Loss: 2.799199, Train Xent: 2.526185, Train Reconst: 0.017315, Train Pn: 0.064737, Train acc 0.333333, Valid Loss: 4.727231, Valid acc 0.337302, Best f1 acc 0.310053,f1 0.310053, acc 0.337302 \n",
      "Epoch 41. Train Loss: 3.252203, Train Xent: 2.528317, Train Reconst: 0.016433, Train Pn: 0.060429, Train acc 0.324561, Valid Loss: 8.214932, Valid acc 0.305556, Best f1 acc 0.310053,f1 0.235979, acc 0.305556 \n",
      "Epoch 42. Train Loss: 2.665125, Train Xent: 2.557096, Train Reconst: 0.015853, Train Pn: 0.060316, Train acc 0.315789, Valid Loss: 52.277702, Valid acc 0.301587, Best f1 acc 0.310053,f1 0.280688, acc 0.301587 \n",
      "Epoch 43. Train Loss: 2.383326, Train Xent: 2.595928, Train Reconst: 0.016888, Train Pn: 0.060177, Train acc 0.307018, Valid Loss: 4.493783, Valid acc 0.341270, Best f1 acc 0.316270,f1 0.316270, acc 0.341270 \n",
      "Epoch 44. Train Loss: 2.662760, Train Xent: 2.469372, Train Reconst: 0.015201, Train Pn: 0.057837, Train acc 0.346491, Valid Loss: 49.545366, Valid acc 0.305556, Best f1 acc 0.316270,f1 0.229365, acc 0.305556 \n",
      "Epoch 45. Train Loss: 2.412478, Train Xent: 2.396665, Train Reconst: 0.016539, Train Pn: 0.058142, Train acc 0.324561, Valid Loss: 4.532112, Valid acc 0.285714, Best f1 acc 0.316270,f1 0.239947, acc 0.285714 \n",
      "Epoch 46. Train Loss: 4.170750, Train Xent: 2.587935, Train Reconst: 0.018417, Train Pn: 0.055692, Train acc 0.328947, Valid Loss: 6.248115, Valid acc 0.289683, Best f1 acc 0.316270,f1 0.209259, acc 0.289683 \n",
      "Epoch 47. Train Loss: 2.906077, Train Xent: 2.611958, Train Reconst: 0.017212, Train Pn: 0.054501, Train acc 0.298246, Valid Loss: 4.745497, Valid acc 0.301587, Best f1 acc 0.316270,f1 0.250661, acc 0.301587 \n",
      "Epoch 48. Train Loss: 3.079691, Train Xent: 2.724688, Train Reconst: 0.015754, Train Pn: 0.053391, Train acc 0.250000, Valid Loss: 5.007482, Valid acc 0.325397, Best f1 acc 0.316270,f1 0.281878, acc 0.325397 \n",
      "Epoch 49. Train Loss: 2.978039, Train Xent: 2.543278, Train Reconst: 0.016907, Train Pn: 0.053089, Train acc 0.315789, Valid Loss: 8.585028, Valid acc 0.325397, Best f1 acc 0.316270,f1 0.281217, acc 0.325397 \n",
      "Epoch 50. Train Loss: 2.246733, Train Xent: 2.565673, Train Reconst: 0.016949, Train Pn: 0.053832, Train acc 0.342105, Valid Loss: 7.500839, Valid acc 0.317460, Best f1 acc 0.316270,f1 0.264418, acc 0.317460 \n",
      "Epoch 51. Train Loss: 2.832061, Train Xent: 2.346537, Train Reconst: 0.016102, Train Pn: 0.051144, Train acc 0.364035, Valid Loss: 13.137037, Valid acc 0.178571, Best f1 acc 0.316270,f1 0.155423, acc 0.178571 \n",
      "Epoch 52. Train Loss: 2.502016, Train Xent: 2.610243, Train Reconst: 0.018177, Train Pn: 0.050708, Train acc 0.324561, Valid Loss: 9.560074, Valid acc 0.226190, Best f1 acc 0.316270,f1 0.180423, acc 0.226190 \n",
      "Epoch 53. Train Loss: 2.059041, Train Xent: 2.416231, Train Reconst: 0.014982, Train Pn: 0.049984, Train acc 0.320175, Valid Loss: 8.569005, Valid acc 0.289683, Best f1 acc 0.316270,f1 0.264286, acc 0.289683 \n",
      "Epoch 54. Train Loss: 2.522171, Train Xent: 2.459074, Train Reconst: 0.016785, Train Pn: 0.050254, Train acc 0.328947, Valid Loss: 21.998169, Valid acc 0.325397, Best f1 acc 0.316270,f1 0.267989, acc 0.325397 \n",
      "Epoch 55. Train Loss: 2.405012, Train Xent: 2.564203, Train Reconst: 0.016630, Train Pn: 0.050424, Train acc 0.333333, Valid Loss: 4.207445, Valid acc 0.257937, Best f1 acc 0.316270,f1 0.214815, acc 0.257937 \n",
      "Epoch 56. Train Loss: 2.548169, Train Xent: 2.368008, Train Reconst: 0.017648, Train Pn: 0.047736, Train acc 0.337719, Valid Loss: 16.299457, Valid acc 0.345238, Best f1 acc 0.316270,f1 0.294180, acc 0.345238 \n",
      "Epoch 57. Train Loss: 2.863823, Train Xent: 2.784127, Train Reconst: 0.016523, Train Pn: 0.045637, Train acc 0.276316, Valid Loss: 3.116894, Valid acc 0.341270, Best f1 acc 0.316270,f1 0.275000, acc 0.341270 \n",
      "Epoch 58. Train Loss: 2.240258, Train Xent: 2.444128, Train Reconst: 0.015511, Train Pn: 0.045107, Train acc 0.346491, Valid Loss: 15.679375, Valid acc 0.305556, Best f1 acc 0.316270,f1 0.231217, acc 0.305556 \n",
      "Epoch 59. Train Loss: 2.791915, Train Xent: 2.630239, Train Reconst: 0.017077, Train Pn: 0.044784, Train acc 0.298246, Valid Loss: 3.331303, Valid acc 0.337302, Best f1 acc 0.316270,f1 0.279762, acc 0.337302 \n",
      "Epoch 60. Train Loss: 3.526988, Train Xent: 2.499039, Train Reconst: 0.014724, Train Pn: 0.042954, Train acc 0.311404, Valid Loss: 25.837464, Valid acc 0.297619, Best f1 acc 0.316270,f1 0.230952, acc 0.297619 \n",
      "Epoch 61. Train Loss: 2.152191, Train Xent: 2.350274, Train Reconst: 0.016982, Train Pn: 0.042304, Train acc 0.342105, Valid Loss: 7.715693, Valid acc 0.301587, Best f1 acc 0.316270,f1 0.259788, acc 0.301587 \n",
      "Epoch 62. Train Loss: 2.210439, Train Xent: 2.482073, Train Reconst: 0.014871, Train Pn: 0.043064, Train acc 0.311404, Valid Loss: 21.092613, Valid acc 0.305556, Best f1 acc 0.316270,f1 0.274868, acc 0.305556 \n",
      "Epoch 63. Train Loss: 2.371551, Train Xent: 2.370878, Train Reconst: 0.015642, Train Pn: 0.043021, Train acc 0.342105, Valid Loss: 5.752351, Valid acc 0.301587, Best f1 acc 0.316270,f1 0.229497, acc 0.301587 \n",
      "Epoch 64. Train Loss: 2.418025, Train Xent: 2.584118, Train Reconst: 0.016234, Train Pn: 0.042379, Train acc 0.328947, Valid Loss: 7.925379, Valid acc 0.162698, Best f1 acc 0.316270,f1 0.143254, acc 0.162698 \n",
      "Epoch 65. Train Loss: 3.131048, Train Xent: 2.408765, Train Reconst: 0.015625, Train Pn: 0.041684, Train acc 0.307018, Valid Loss: 14.058045, Valid acc 0.301587, Best f1 acc 0.316270,f1 0.233466, acc 0.301587 \n",
      "Epoch 66. Train Loss: 2.118842, Train Xent: 2.564740, Train Reconst: 0.015618, Train Pn: 0.042575, Train acc 0.328947, Valid Loss: 3.314093, Valid acc 0.329365, Best f1 acc 0.316270,f1 0.296032, acc 0.329365 \n",
      "Epoch 67. Train Loss: 2.357766, Train Xent: 2.417297, Train Reconst: 0.015120, Train Pn: 0.042460, Train acc 0.399123, Valid Loss: 5.120488, Valid acc 0.301587, Best f1 acc 0.316270,f1 0.237963, acc 0.301587 \n",
      "Epoch 68. Train Loss: 1.964288, Train Xent: 2.578728, Train Reconst: 0.015121, Train Pn: 0.040813, Train acc 0.355263, Valid Loss: 12.388738, Valid acc 0.353175, Best f1 acc 0.316270,f1 0.294180, acc 0.353175 \n",
      "Epoch 69. Train Loss: 2.704579, Train Xent: 2.524618, Train Reconst: 0.015643, Train Pn: 0.038999, Train acc 0.315789, Valid Loss: 4.105439, Valid acc 0.333333, Best f1 acc 0.316270,f1 0.304762, acc 0.333333 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70. Train Loss: 2.191177, Train Xent: 2.396460, Train Reconst: 0.015215, Train Pn: 0.039288, Train acc 0.328947, Valid Loss: 13.241214, Valid acc 0.317460, Best f1 acc 0.316270,f1 0.302910, acc 0.317460 \n",
      "Epoch 71. Train Loss: 2.272240, Train Xent: 2.521175, Train Reconst: 0.016584, Train Pn: 0.037985, Train acc 0.333333, Valid Loss: 14.464527, Valid acc 0.273810, Best f1 acc 0.316270,f1 0.225000, acc 0.273810 \n",
      "Epoch 72. Train Loss: 2.109194, Train Xent: 2.364879, Train Reconst: 0.014938, Train Pn: 0.038272, Train acc 0.324561, Valid Loss: 6.535020, Valid acc 0.297619, Best f1 acc 0.316270,f1 0.273016, acc 0.297619 \n",
      "Epoch 73. Train Loss: 2.409133, Train Xent: 2.592926, Train Reconst: 0.014622, Train Pn: 0.036958, Train acc 0.289474, Valid Loss: 4.445194, Valid acc 0.293651, Best f1 acc 0.316270,f1 0.246032, acc 0.293651 \n",
      "Epoch 74. Train Loss: 2.382578, Train Xent: 2.456031, Train Reconst: 0.015951, Train Pn: 0.037167, Train acc 0.328947, Valid Loss: 3.372244, Valid acc 0.361111, Best f1 acc 0.316270,f1 0.306349, acc 0.361111 \n",
      "Epoch 75. Train Loss: 1.959620, Train Xent: 2.415762, Train Reconst: 0.015325, Train Pn: 0.037175, Train acc 0.293860, Valid Loss: 15.621339, Valid acc 0.301587, Best f1 acc 0.316270,f1 0.229365, acc 0.301587 \n",
      "Epoch 76. Train Loss: 1.812507, Train Xent: 2.546471, Train Reconst: 0.014938, Train Pn: 0.036680, Train acc 0.307018, Valid Loss: 4.103607, Valid acc 0.289683, Best f1 acc 0.316270,f1 0.236376, acc 0.289683 \n",
      "Epoch 77. Train Loss: 2.031865, Train Xent: 2.375224, Train Reconst: 0.015386, Train Pn: 0.036389, Train acc 0.350877, Valid Loss: 4.747166, Valid acc 0.301587, Best f1 acc 0.316270,f1 0.251323, acc 0.301587 \n",
      "Epoch 78. Train Loss: 1.956916, Train Xent: 2.351455, Train Reconst: 0.016383, Train Pn: 0.035222, Train acc 0.346491, Valid Loss: 54.822211, Valid acc 0.166667, Best f1 acc 0.316270,f1 0.134392, acc 0.166667 \n",
      "Epoch 79. Train Loss: 1.961114, Train Xent: 2.456011, Train Reconst: 0.015162, Train Pn: 0.034910, Train acc 0.324561, Valid Loss: 4.926642, Valid acc 0.301587, Best f1 acc 0.316270,f1 0.231746, acc 0.301587 \n",
      "Epoch 80. Train Loss: 2.121932, Train Xent: 2.570428, Train Reconst: 0.016964, Train Pn: 0.033878, Train acc 0.289474, Valid Loss: 19.101308, Valid acc 0.293651, Best f1 acc 0.316270,f1 0.220899, acc 0.293651 \n",
      "Epoch 81. Train Loss: 2.261021, Train Xent: 2.586627, Train Reconst: 0.015000, Train Pn: 0.033988, Train acc 0.276316, Valid Loss: 5.115255, Valid acc 0.317460, Best f1 acc 0.316270,f1 0.258333, acc 0.317460 \n"
     ]
    }
   ],
   "source": [
    "# main training loop\n",
    "# def train(net, train_loader,labeled_loader, eval_loader, num_epochs, wd):\n",
    "\n",
    "wd = 5e-4\n",
    "num_epochs = 1000\n",
    "best_f1 = 0\n",
    "for epoch in range(30,num_epochs):\n",
    "    train_loss = 0; train_loss_xentropy = 0; train_loss_reconst = 0; train_loss_pn = 0; train_loss_kl = 0; train_loss_bnmm = 0\n",
    "    correct = 0\n",
    "    num_batch_train = 0\n",
    "\n",
    "    # start with adam optimizer but switch sgd optimizer with exponential decay learning rate since epoch 20 \n",
    "    if epoch == 250:\n",
    "        sgd_lr = opt.lr[1]\n",
    "        decay_val = np.exp(np.log(sgd_lr / 0.0001) / (num_epochs - 2))\n",
    "        sgd_lr = sgd_lr * decay_val\n",
    "        trainer = th.optim.SGD(net.parameters(), sgd_lr, weight_decay=wd)\n",
    "\n",
    "    if epoch >= 250:\n",
    "        for param_group in trainer.param_groups:\n",
    "            param_group['lr'] = param_group['lr']/decay_val\n",
    "\n",
    "    for param_group in trainer.param_groups:\n",
    "        learning_rate = param_group['lr']\n",
    "\n",
    "    meters = AverageMeterSet()\n",
    "\n",
    "    # switch to train mode\n",
    "    net.train()\n",
    "\n",
    "#     end = time.time()\n",
    "    for i in range(int(n_iterations)):\n",
    "#         meters.update('data_time', time.time() - end)\n",
    "\n",
    "        unsup_batch = next(iter(train_loader))\n",
    "        sup_batch,target = next(iter(labeled_loader))\n",
    "\n",
    "        # set up unlabeled input and labeled input with the corresponding labels\n",
    "        input_unsup_var = th.autograd.Variable(unsup_batch[0:(opt.batch_size - opt.labeled_batch_size)]).to(device)\n",
    "        input_sup_var = th.autograd.Variable(sup_batch).to(device)\n",
    "        target_sup_var = th.autograd.Variable(target.data.long()).to(device)\n",
    "\n",
    "        minibatch_unsup_size = opt.batch_size - opt.labeled_batch_size\n",
    "        minibatch_sup_size = opt.labeled_batch_size\n",
    "\n",
    "        # compute loss for unlabeled input\n",
    "        [output_unsup, xhat_unsup, loss_pn_unsup, loss_bnmm_unsup] = net(input_unsup_var)\n",
    "        loss_reconst_unsup = L2_loss(xhat_unsup, input_unsup_var).mean()\n",
    "        softmax_unsup = F.softmax(output_unsup)\n",
    "        loss_kl_unsup = -th.sum(th.log(10.0*softmax_unsup + 1e-8) * softmax_unsup) / minibatch_unsup_size\n",
    "        loss_unsup = opt.alpha_reconst * loss_reconst_unsup + opt.alpha_kl * loss_kl_unsup + opt.alpha_bnmm * loss_bnmm_unsup + opt.alpha_pn * loss_pn_unsup\n",
    "\n",
    "\n",
    "\n",
    "        # compute loss for labeled input\n",
    "        [output_sup, xhat_sup, loss_pn_sup, loss_bnmm_sup] = net(input_sup_var, target_sup_var.squeeze_())\n",
    "        loss_xentropy_sup = criterion(output_sup, target_sup_var) / minibatch_sup_size\n",
    "        loss_reconst_sup = L2_loss(xhat_sup, input_sup_var).mean()\n",
    "        softmax_sup = F.softmax(output_sup)\n",
    "        loss_kl_sup = -th.sum(th.log(10.0*softmax_sup + 1e-8) * softmax_sup)/ minibatch_sup_size\n",
    "        loss_sup = loss_xentropy_sup + opt.alpha_reconst * loss_reconst_sup + opt.alpha_kl * loss_kl_sup + opt.alpha_bnmm * loss_bnmm_sup + opt.alpha_pn * loss_pn_sup\n",
    "\n",
    "        loss = th.mean(loss_unsup + loss_sup)\n",
    "\n",
    "        # compute the grads and update the parameters\n",
    "        trainer.zero_grad()\n",
    "        loss.backward()\n",
    "        trainer.step()\n",
    "\n",
    "        # accumulate all the losses for visualization\n",
    "        loss_reconst = loss_reconst_unsup + loss_reconst_sup\n",
    "        loss_pn = loss_pn_unsup + loss_pn_sup\n",
    "        loss_xentropy = loss_xentropy_sup\n",
    "        loss_kl = loss_kl_unsup + loss_kl_sup\n",
    "        loss_bnmm = loss_bnmm_unsup + loss_bnmm_sup\n",
    "\n",
    "        train_loss_xentropy += th.mean(loss_xentropy).cpu().detach().numpy()\n",
    "        train_loss_reconst += th.mean(loss_reconst).cpu().detach().numpy()\n",
    "        train_loss_pn += th.mean(loss_pn).cpu().detach().numpy()\n",
    "        train_loss_kl += th.mean(loss_kl).cpu().detach().numpy()\n",
    "        train_loss_bnmm += th.mean(loss_bnmm).cpu().detach().numpy()\n",
    "        train_loss += th.mean(loss).cpu().detach().numpy()\n",
    "        correct += get_acc(output_sup, target_sup_var).cpu().detach().numpy()\n",
    "\n",
    "        num_batch_train += 1\n",
    "        iter_indx += 1\n",
    "    writer.add_scalars('loss', {'train': train_loss / num_batch_train}, epoch)\n",
    "    writer.add_scalars('loss_xentropy', {'train': train_loss_xentropy / num_batch_train}, epoch)\n",
    "    writer.add_scalars('loss_reconst', {'train': train_loss_reconst / num_batch_train}, epoch)\n",
    "    writer.add_scalars('loss_pn', {'train': train_loss_pn / num_batch_train}, epoch)\n",
    "    writer.add_scalars('loss_kl', {'train': train_loss_kl / num_batch_train}, epoch)\n",
    "    writer.add_scalars('loss_bnmm', {'train': train_loss_bnmm / num_batch_train}, epoch)\n",
    "    writer.add_scalars('acc', {'train': correct / num_batch_train}, epoch)\n",
    "\n",
    "    cur_time = datetime.datetime.now()\n",
    "    h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "    m, s = divmod(remainder, 60)\n",
    "    time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "\n",
    "    # Validation\n",
    "    valid_loss = 0; valid_loss_xentropy = 0; valid_loss_reconst = 0; valid_loss_pn = 0; valid_loss_kl = 0; valid_loss_bnmm = 0\n",
    "    valid_correct = 0\n",
    "    num_batch_valid = 0\n",
    "    valid_accuracy = 0\n",
    "    valid_f1 = 0\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    for i, (batch, target) in enumerate(eval_loader):\n",
    "        with th.no_grad():\n",
    "            input_var = th.autograd.Variable(batch).to(device)\n",
    "            target_var = th.autograd.Variable(target.data.long()).to(device)\n",
    "\n",
    "            minibatch_size = len(target_var)\n",
    "\n",
    "            [output, xhat, loss_pn, loss_bnmm] = net(input_var, target_var)\n",
    "\n",
    "            loss_xentropy = criterion(output, target_var.squeeze_())/minibatch_size\n",
    "            loss_reconst = L2_loss(xhat, input_var).mean()\n",
    "            softmax_val = F.softmax(output)\n",
    "            loss_kl = -th.sum(th.log(10.0*softmax_val + 1e-8) * softmax_val)/minibatch_size\n",
    "            loss = loss_xentropy + opt.alpha_reconst * loss_reconst + opt.alpha_kl * loss_kl + opt.alpha_bnmm * loss_bnmm + opt.alpha_pn * loss_pn\n",
    "\n",
    "            valid_loss_xentropy += th.mean(loss_xentropy).cpu().detach().numpy()\n",
    "            valid_loss_reconst += th.mean(loss_reconst).cpu().detach().numpy()\n",
    "            valid_loss_pn += th.mean(loss_pn).cpu().detach().numpy()\n",
    "            valid_loss_kl += th.mean(loss_kl).cpu().detach().numpy()\n",
    "            valid_loss_bnmm += th.mean(loss_bnmm).cpu().detach().numpy()\n",
    "            valid_loss += th.mean(loss).cpu().detach().numpy()\n",
    "            valid_correct += get_acc(output, target_var).cpu().detach().numpy()\n",
    "            \n",
    "            accuracy, f1 = compute_metrics(target_var.cpu(), th.argmax(output, dim=1, keepdim=False).cpu())\n",
    "            valid_accuracy+=accuracy\n",
    "            valid_f1+=f1\n",
    "            num_batch_valid += 1\n",
    "\n",
    "    valid_acc = valid_correct / num_batch_valid\n",
    "    f1_s = valid_f1/num_batch_valid\n",
    "    if f1_s > best_f1:\n",
    "        best_f1 = f1_s\n",
    "        \n",
    "#         th.save(net.state_dict(), '%s/%s_best.pth'%(opt.model_dir, opt.exp_name))\n",
    "#     writer.add_scalars('loss', {'valid': valid_loss / num_batch_valid}, epoch)\n",
    "#     writer.add_scalars('loss_xentropy', {'valid': valid_loss_xentropy / num_batch_valid}, epoch)\n",
    "#     writer.add_scalars('loss_reconst', {'valid': valid_loss_reconst / num_batch_valid}, epoch)\n",
    "#     writer.add_scalars('loss_pn', {'valid': valid_loss_pn / num_batch_valid}, epoch)\n",
    "#     writer.add_scalars('loss_kl', {'valid': valid_loss_kl / num_batch_valid}, epoch)\n",
    "#     writer.add_scalars('loss_bnmm', {'valid': valid_loss_bnmm / num_batch_valid}, epoch)\n",
    "#     writer.add_scalars('acc', {'valid': valid_acc}, epoch)\n",
    "    epoch_str = (\"Epoch %d. Train Loss: %f, Train Xent: %f, Train Reconst: %f, Train Pn: %f, Train acc %f, Valid Loss: %f, Valid acc %f, Best f1 acc %f,f1 %f, acc %f \"\n",
    "                 % (epoch, train_loss / num_batch_train, train_loss_xentropy / num_batch_train, train_loss_reconst / num_batch_train, train_loss_pn / num_batch_train,\n",
    "                    correct / num_batch_train, valid_loss / num_batch_valid, valid_acc, best_f1,valid_f1/num_batch_valid,valid_accuracy/num_batch_valid))\n",
    "    if not epoch % 20:\n",
    "        th.save(net.state_dict(), '%s/%s_epoch_%i.pth'%(opt.model_dir, opt.exp_name, epoch))\n",
    "\n",
    "    prev_time = cur_time\n",
    "#     logging.info(epoch_str + time_str + ', lr ' + str(learning_rate))\n",
    "    print(epoch_str)   \n",
    "#     return best_valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from utils.constants import Constants\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        th.nn.init.xavier_uniform(m.weight)\n",
    "        # m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Bias') != -1:\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "#     acc = train(model, train_loader, eval_loader, opt.num_epochs, opt.weight_decay)\n",
    "#     logging.info('Validation Accuracy - Run %i = %f'%(i, acc))\n",
    "#     valid_acc += acc\n",
    "\n",
    "#     logging.info('Validation Accuracy = %f'%(valid_acc/num_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "\n",
    "cfg = {\n",
    "    'AllConv13': [128, 128, 128, 'M', 256, 256, 256, 'M', 512, 256, 128, 'A'],\n",
    "}\n",
    "\n",
    "#################### Some utils class ####################\n",
    "class Reshape(nn.Module):\n",
    "    \"\"\"\n",
    "    Flatten the output of the convolutional layer\n",
    "    Parameters\n",
    "    ----------\n",
    "    Input shape: (N, C * W * H)\n",
    "    Output shape: (N, C, W, H)\n",
    "    \"\"\"\n",
    "    def __init__(self, shape, **kwargs):\n",
    "        super(Reshape, self).__init__(**kwargs)\n",
    "        self._shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.reshape(x.size()[0], self._shape[0], self._shape[1], self._shape[2])\n",
    "    \n",
    "class BiasAdder(nn.Module):\n",
    "    \"\"\"\n",
    "    Add a bias into the input\n",
    "    \"\"\"\n",
    "    def __init__ (self, channels, **kwargs):\n",
    "        super(BiasAdder, self).__init__(**kwargs)\n",
    "        self.bias = nn.Parameter(th.Tensor(1,channels,1,1))\n",
    "        self.bias.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.bias\n",
    "    \n",
    "class Flatten(nn.Module):\n",
    "    \"\"\"\n",
    "    Flatten 4D tensor into 2D tensor\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "    \n",
    "class Upaverage(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsample to reverse the avg pooling layer\n",
    "    \"\"\"\n",
    "    def __init__(self, scale_factor, **kwargs):\n",
    "        super(Upaverage, self).__init__(**kwargs)\n",
    "        self.scale_factor = scale_factor\n",
    "        self.upsample_layer = nn.Upsample(scale_factor=self.scale_factor, mode='nearest')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.upsample_layer(x) * (1./self.scale_factor)**2\n",
    "    \n",
    "def make_one_hot(labels, C=2):\n",
    "    \"\"\"\n",
    "    Converts an integer label torch.autograd.Variable to a one-hot Variable.\n",
    "    \"\"\"\n",
    "    target = th.eye(C)[labels.data]\n",
    "    target = target.to(labels.get_device())      \n",
    "    return target\n",
    "\n",
    "#################### Main NRM class ####################\n",
    "class NRM(nn.Module):\n",
    "    def __init__(self, net_name, batch_size, num_class, use_bias=False, use_bn=False, do_topdown=False, do_pn=False, do_bnmm=False):\n",
    "        super(NRM, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.do_topdown = do_topdown\n",
    "        self.do_pn = do_pn\n",
    "        self.do_bnmm = do_bnmm\n",
    "        self.use_bn = use_bn\n",
    "        self.use_bias = use_bias\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # create:\n",
    "        # feature extractor in the forward cnn step: self.features\n",
    "        # corresponding layer inm the top-down reconstruction nrm step: layers_nrm\n",
    "        # instance norm used in the top-down reconstruction nrm step: insnorms_nrm\n",
    "        # instance norm used in the forward cnn step: insnorms_cnn\n",
    "        self.features, layers_nrm, insnorms_nrm, insnorms_cnn = self._make_layers(cfg[net_name], use_bias, use_bn, self.do_topdown)\n",
    "        \n",
    "        # create the classifer in the forward cnn step\n",
    "        conv_layer = nn.Conv2d(in_channels=cfg[net_name][-2], out_channels=self.num_class, kernel_size=(1,1), bias=True)\n",
    "        flatten_layer = Flatten()\n",
    "        self.classifier = nn.Sequential(OrderedDict([('conv',conv_layer), ('flatten', flatten_layer)]))\n",
    "        \n",
    "        # create the nrm\n",
    "        if self.do_topdown:\n",
    "            # add layers corresponding to the classifer in the forward step\n",
    "            convtd_layer = nn.ConvTranspose2d(out_channels=cfg[net_name][-2], in_channels=self.num_class, kernel_size=(1,1), stride=(1, 1), bias=False)\n",
    "            convtd_layer.weight.data = conv_layer.weight.data\n",
    "            layers_nrm += [('convtd',convtd_layer), ('reshape', Reshape(shape=(self.num_class, 1, 1)))]\n",
    "            \n",
    "            self.nrm = nn.Sequential(OrderedDict(layers_nrm[::-1]))\n",
    "            \n",
    "            # if use path normalization, then also use instance normalization\n",
    "            if self.do_pn:\n",
    "                self.insnorms_nrm = nn.Sequential(OrderedDict(insnorms_nrm[::-1]))\n",
    "                self.insnorms_cnn = nn.Sequential(OrderedDict(insnorms_cnn))\n",
    "\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        ahat = []; that = []; bcnn = []; apn = []; meancnn = []; varcnn = []\n",
    "        xbias = th.zeros([1, x.shape[1], x.shape[2], x.shape[3]], device=x.get_device()) if self.do_pn else []\n",
    "        insnormcnn_indx = 0\n",
    "        \n",
    "        # if do top-down reconstruction, we need to keep track of relu state, maxpool state,\n",
    "        # mean and var of the activations, and the bias terms in the forward cnn step\n",
    "        if self.do_topdown: \n",
    "            for name, layer in self.features.named_children():\n",
    "                if name.find('pool') != -1 and not name.find('average') != -1: # keep track of the maxpool state\n",
    "                    F.interpolate(layer(x), scale_factor=2, mode='nearest')\n",
    "                    that.append(th.gt(x-F.interpolate(layer(x), scale_factor=2, mode='nearest'),0))\n",
    "                    x = layer(x)\n",
    "                    if self.do_pn:\n",
    "                        xbias = layer(xbias)\n",
    "                else:\n",
    "                    x = layer(x)\n",
    "                    \n",
    "                    if self.do_pn: # get the forward results to compute the path normalization later\n",
    "                        if name.find('batchnorm') != -1:\n",
    "                            xbias = self.insnorms_cnn[insnormcnn_indx](xbias)\n",
    "                            insnormcnn_indx += 1\n",
    "                        else:\n",
    "                            xbias = layer(xbias)\n",
    "                    if name.find('relu') != -1: # keep track of the relu state\n",
    "                        ahat.append(th.gt(x,0) + th.le(x,0)*0.1)\n",
    "                        if self.do_pn:\n",
    "                            apn.append(th.gt(xbias,0) + th.le(xbias,0)*0.1)\n",
    "                    \n",
    "                    if self.use_bn:\n",
    "                        if name.find('conv') != -1: # keep track of the mean and var of the activations\n",
    "                            meancnn.append(th.mean(x, dim=(0,2,3), keepdim=True))\n",
    "                            varcnn.append(th.mean((x - th.mean(x, dim=(0,2,3), keepdim=True))**2, dim=(0,2,3), keepdim=True))\n",
    "                        if self.use_bias: # keep track of the bias terms when adding bias\n",
    "                            if name.find('bias') != -1: \n",
    "                                bcnn.append(layer.bias)\n",
    "                        else: # otherwise, keep track of the bias terms inside the batch norm\n",
    "                            if name.find('batchnorm') != -1:\n",
    "                                bcnn.append(layer.bias)\n",
    "                    else:\n",
    "                        if self.use_bias:\n",
    "                            if name.find('conv') != -1:\n",
    "                                bcnn.append(layer.bias)\n",
    "            \n",
    "            # reverse the order of the parameters/variables that we keep track to use in the top-down reconstruction nrm step since nrm is the reverse of cnn\n",
    "            ahat = ahat[::-1]\n",
    "            that = that[::-1]\n",
    "            bcnn = bcnn[::-1]\n",
    "            apn = apn[::-1]\n",
    "            meancnn = meancnn[::-1]\n",
    "            varcnn = varcnn[::-1]\n",
    "        else:\n",
    "            x =  self.features(x)\n",
    "        \n",
    "        # send the features into the classifier\n",
    "        z = self.classifier(x)\n",
    "        \n",
    "        # do reconstruction via nrm\n",
    "        # xhat: the reconstruction image\n",
    "        # loss_pn: path normalization loss\n",
    "        # loss_bnmm: batch norm moment matching loss\n",
    "        if self.do_topdown:\n",
    "            xhat, _, loss_pn, loss_bnmm = self.topdown(self.nrm, make_one_hot(y, self.num_class), ahat, that, bcnn, th.ones([1, z.size()[1]], device=z.get_device()), apn, meancnn, varcnn) if y is not None else self.topdown(self.nrm, make_one_hot(th.argmax(z.detach(), dim=1), self.num_class), ahat, that, bcnn, th.ones([1, z.size()[1]], device=z.get_device()), apn, meancnn, varcnn)\n",
    "        else:\n",
    "            xhat = None\n",
    "            loss_pn = None\n",
    "            loss_bnmm = None\n",
    "\n",
    "\n",
    "        return [z, xhat, loss_pn, loss_bnmm]\n",
    "\n",
    "    def _make_layers(self, cfg, use_bias, use_bn, do_topdown):\n",
    "        layers = []\n",
    "        layers_nrm = []\n",
    "        insnorms_nrm = []\n",
    "        insnorms_cnn = []\n",
    "        in_channels = 3\n",
    "\n",
    "        for i, x in enumerate(cfg):\n",
    "            if x == 'M': # if max pooling layer, then add max pooling and dropout into the cnn. Add upsample layers, dropout, batchnorm, and instance norm - for path normaliztion - into the nrm.\n",
    "                layers += [('pool%i'%i, nn.MaxPool2d(2, stride=2)), ('dropout%i'%i, nn.Dropout(0.5))]\n",
    "                if do_topdown:\n",
    "                    if use_bn:\n",
    "                        layers_nrm += [('upsample%i'%i, nn.Upsample(scale_factor=2, mode='nearest')), ('dropout%i'%i, nn.Dropout(0.5)), ('batchnorm%i'%i, nn.BatchNorm2d(cfg[i-1]))]\n",
    "                        insnorms_nrm += [('instancenormtd%i'%i, nn.InstanceNorm2d(cfg[i-1], affine=True))]\n",
    "                    else:\n",
    "                        layers_nrm += [('upsample%i'%i, nn.Upsample(scale_factor=2, mode='nearest')), ('dropout%i'%i, nn.Dropout(0.5))]\n",
    "                        \n",
    "            elif x == 'A': # if avg pooling layer, then add average pooling layer into the cnn. Add up average layers, batchnorm and instance norm - for path normaliztion - into the nrm.\n",
    "                layers += [('average%i'%i, nn.AvgPool2d(6, stride=1))]\n",
    "                if do_topdown:\n",
    "                    if use_bn:\n",
    "                        layers_nrm += [('upaverage%i'%i, Upaverage(scale_factor=6)), ('batchnorm%i'%i, nn.BatchNorm2d(cfg[i-1]))]\n",
    "                        insnorms_nrm += [('instancenormtd%i'%i, nn.InstanceNorm2d(cfg[i-1], affine=True))]\n",
    "                    else:\n",
    "                        layers_nrm += [('upaverage%i'%i, Upaverage(scale_factor=6))]\n",
    "                        \n",
    "            else: # add other layers into the cnn and the nrm\n",
    "                padding_cnn = (0,0) if x == 512 else (1,1)\n",
    "                padding_nrm = (0,0) if x == 512 else (1,1)\n",
    "                if use_bn:\n",
    "                    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=x, kernel_size=(3,3), padding=padding_cnn, bias=False)\n",
    "                    if use_bias:\n",
    "                        layers += [('conv%i'%i, conv_layer),\n",
    "                                   ('batchnorm%i'%i, nn.BatchNorm2d(x)),\n",
    "                                   ('bias%i'%i,BiasAdder(channels=x)),\n",
    "                                   ('relu%i'%i,nn.LeakyReLU(0.1))]\n",
    "                    else:\n",
    "                        layers += [('conv%i'%i, conv_layer),\n",
    "                                   ('batchnorm%i'%i, nn.BatchNorm2d(x)),\n",
    "                                   ('relu%i'%i, nn.LeakyReLU(0.1))]\n",
    "                    \n",
    "                    insnorms_cnn += [('instancenormcnn%i'%i, nn.InstanceNorm2d(x, affine=True))]\n",
    "                    if do_topdown:\n",
    "                        if (cfg[i-1] == 'M' or cfg[i-1] == 'A') and not i == 0:\n",
    "                            layers_nrm += [('convtd%i'%i, nn.ConvTranspose2d(out_channels=in_channels, in_channels=x, kernel_size=3, stride=(1, 1),\n",
    "                                                              padding=padding_nrm, bias=False))]\n",
    "                            layers_nrm[-1][-1].weight.data = conv_layer.weight.data\n",
    "                        else:\n",
    "                            layers_nrm += [('batchnormtd%i'%i, nn.BatchNorm2d(in_channels)), ('convtd%i'%i, nn.ConvTranspose2d(out_channels=in_channels, in_channels=x, kernel_size=3, stride=(1, 1), padding=padding_nrm, bias=False))]\n",
    "                            layers_nrm[-1][-1].weight.data = conv_layer.weight.data\n",
    "                            insnorms_nrm += [('instancenormtd%i'%i, nn.InstanceNorm2d(in_channels, affine=True))]\n",
    "                    \n",
    "                elif use_bias:\n",
    "                    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=x, kernel_size=(3,3), padding=padding_cnn, use_bias=True)\n",
    "                    layers += [('conv%i'%i, conv_layer), ('relu%i'%i, nn.LeakyReLU(0.1))]\n",
    "                    if do_topdown:\n",
    "                        layers_nrm += [('convtd%i'%i, nn.ConvTranspose2d(out_channels=in_channels, in_channels=x, kernel_size=3, stride=(1, 1),\n",
    "                                                          padding=padding_nrm, bias=False))]\n",
    "                        layers_nrm[-1][-1].weight.data = conv_layer.weight.data\n",
    "                    \n",
    "                else:\n",
    "                    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=x, kernel_size=(3,3), padding=padding_cnn, bias=False)\n",
    "                    layers += [('conv%i'%i, conv_layer), ('relu%i'%i, nn.LeakyReLU(0.1))]\n",
    "                    if do_topdown:\n",
    "                        layers_nrm += [('convtd%i'%i, nn.ConvTranspose2d(out_channels=in_channels, in_channels=x, kernel_size=3, stride=(1,1),\n",
    "                                                          padding=padding_nrm, bias=False))]\n",
    "                        layers_nrm[-1][-1].weight.data = conv_layer.weight.data\n",
    "                        \n",
    "                in_channels = x\n",
    "\n",
    "        model = nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "        return model, layers_nrm, insnorms_nrm, insnorms_cnn\n",
    "\n",
    "    def topdown(self, net, xhat, ahat, that, bcnn, xpn, apn, meancnn, varcnn):\n",
    "        mu = xhat\n",
    "        mupn = xpn\n",
    "        loss_pn = th.zeros([self.batch_size,], device=mu.get_device())\n",
    "        loss_bnmm = th.zeros([self.batch_size,], device=mu.get_device())\n",
    "\n",
    "        ahat_indx = 0; that_indx = 0; meanvar_indx = 0; insnormtd_indx = 0\n",
    "        prev_name = ''\n",
    "        \n",
    "        for i, (name, layer) in enumerate(net.named_children()):\n",
    "            if name.find('conv') != -1 and i > 1: \n",
    "                mu = mu * ahat[ahat_indx].type(th.FloatTensor).to(mu.get_device()) # mask the intermediate rendered images by the relu states in the forward step\n",
    "                \n",
    "                if self.do_pn: # compute the path normalization loss\n",
    "                    mupn = mupn * apn[ahat_indx].type(th.FloatTensor).to(mu.get_device())\n",
    "                    mu_b = bcnn[ahat_indx].data.reshape((1, -1, 1, 1)) * mu\n",
    "                    mupn_b = bcnn[ahat_indx].data.reshape((1, -1, 1, 1)) * mupn\n",
    "                    \n",
    "                    loss_pn_layer = th.mean(th.abs(mu_b - mupn_b), dim=(1,2,3))\n",
    "                    loss_pn = loss_pn + loss_pn_layer\n",
    "\n",
    "                ahat_indx += 1\n",
    "\n",
    "            if prev_name.find('upsamplelayer') != -1 and not prev_name.find('avg') != -1:\n",
    "                mu = mu * that[that_indx].type(th.FloatTensor).to(mu.get_device()) # mask the intermediate rendered images by the maxpool states in the forward step\n",
    "                if self.do_pn:\n",
    "                    mupn = mupn * that[that_indx].type(th.FloatTensor).to(mu.get_device())\n",
    "                that_indx += 1\n",
    "          \n",
    "            # compute the next intermediate rendered images\n",
    "            mu = layer(mu)\n",
    "            \n",
    "            # compute the next intermediate rendered results for computing the path normalization loss in the next layer\n",
    "            if (name.find('batchnorm') != -1) and (i < len(net) - 1):\n",
    "                if self.do_pn:\n",
    "                    mupn = self.insnorms_nrm[insnormtd_indx](mupn)\n",
    "                    insnormtd_indx += 1\n",
    "            else:\n",
    "                if self.do_pn:\n",
    "                    mupn = layer(mupn)\n",
    "            \n",
    "            if (name.find('conv') != -1) and (i != (len(net)-2)):\n",
    "                if self.do_bnmm and self.use_bn:\n",
    "                    # compute the KL distance between two Gaussians - the intermediate rendered images and the mean/var from the forward step\n",
    "                    loss_bnmm = loss_bnmm + 0.5*th.mean(((th.mean(mu, dim=(0,2,3)) - meancnn[meanvar_indx])**2)/varcnn[meanvar_indx]) + 0.5*th.mean(th.mean((mu - th.mean(mu, dim=(0,2,3), keepdim=True))**2, dim=(0,2,3))/varcnn[meanvar_indx]) - 0.5*th.mean(th.log(th.mean((mu - th.mean(mu, dim=(0,2,3), keepdim=True))**2, dim=(0,2,3)) + 1e-8) - th.log(varcnn[meanvar_indx])) - 0.5\n",
    "                    meanvar_indx += 1\n",
    "                    \n",
    "            prev_name = name\n",
    "            \n",
    "        return mu, mupn, loss_pn, loss_bnmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
