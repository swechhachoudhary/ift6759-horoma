{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import logging\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "import torchvision.datasets\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import shutil\n",
    "import _pickle as cPickle\n",
    "from sklearn import preprocessing\n",
    "import subprocess\n",
    "\n",
    "from IPython.core.debugger import Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.seed_val = 0 # random seed val\n",
    "        self.num_train_sup = 228 # number of labeled train samples\n",
    "        self.batch_size = 6 # batch size\n",
    "        self.labeled_batch_size = 3 # number of labeled samples in a batch\n",
    "        self.device = 0 # gpu id\n",
    "        \n",
    "        self.lr = [0.001, 0.15] # learning rate for adam and then and then initial learning rate for sgd respectively\n",
    "        self.num_epochs = 500 # number of training epochs\n",
    "        self.weight_decay = 5e-4 # weight decay\n",
    "        \n",
    "        self.mount_point = '/home/onu/Desktop/' # change this to your mount_point\n",
    "        self.datadir = 'data-local/images/cifar/cifar10/by-image' # dataset directory\n",
    "        self.labels = 'data-local/labels/cifar10/4000_balanced_labels/00.txt' # label directory\n",
    "        self.log_dir = os.path.join(self.mount_point,'logs') # log directory\n",
    "        self.model_dir = os.path.join(self.mount_point,'models') # log \n",
    "        self.exp_name = 'cifar10_nl_%i_allconv13_seed_%i'%(self.num_train_sup, self.seed_val) # name of experiments\n",
    "        \n",
    "        self.train_subdir = 'train+val'\n",
    "        self.eval_subdir = 'test'\n",
    "        self.num_classes = 17\n",
    "        self.workers = 4\n",
    "        \n",
    "        self.alpha_reconst = 0.5 # weight for reconstruction loss\n",
    "        self.alpha_pn = 1.0 # weight for path normalization loss\n",
    "        self.alpha_kl = 0.5 # weight for kl loss\n",
    "        self.alpha_bnmm = 0.5 # weight for moment matching loss when doing batchnorm\n",
    "        \n",
    "        self.use_bias = True # add bias after batchnorm\n",
    "        self.use_bn = True # use batch norm\n",
    "        self.do_topdown = True # do topdown\n",
    "        self.do_pn = True # do path normalization\n",
    "        self.do_bnmm = True # do moment matching for batchnorm\n",
    "\n",
    "opt = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "th.cuda.set_device(opt.device)\n",
    "device = th.device(\"cuda:%i\"%opt.device if (opt.device >= 0) else \"cpu\")\n",
    "\n",
    "def gpu_device(device=0):\n",
    "    try:\n",
    "        _ = th.tensor([1, 2, 3], device=th.device('cuda', device))\n",
    "    except ValueError:\n",
    "        return None\n",
    "    return th.device('cuda', device)\n",
    "\n",
    "assert gpu_device(opt.device), 'No GPU device found!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make required folders\n",
    "if not os.path.exists(opt.log_dir):\n",
    "    os.makedirs(opt.log_dir)\n",
    "log_dir = os.path.join(opt.log_dir, opt.exp_name)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "if not os.path.exists(opt.model_dir):\n",
    "    os.makedirs(opt.model_dir)\n",
    "if not os.path.exists(os.path.join(opt.mount_point,'datasets')):\n",
    "    os.makedirs(os.path.join(opt.mount_point,'datasets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set logging option\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "console = logging.StreamHandler()\n",
    "console.setFormatter(formatter)\n",
    "logger.addHandler(console)\n",
    "\n",
    "hdlr = logging.FileHandler(os.path.join(opt.log_dir, '{}.log'.format(opt.exp_name)))\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logging.info(opt)\n",
    "\n",
    "writer = SummaryWriter(os.path.join(opt.log_dir, opt.exp_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set losses\n",
    "NO_LABEL = -1\n",
    "criterion = nn.CrossEntropyLoss(size_average=False, ignore_index=NO_LABEL).cuda()\n",
    "L2_loss = nn.MSELoss(size_average=False, reduce=False, reduction='mean').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some util functions\n",
    "\n",
    "\n",
    "class AverageMeterSet:\n",
    "    def __init__(self):\n",
    "        self.meters = {}\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.meters[key]\n",
    "\n",
    "    def update(self, name, value, n=1):\n",
    "        if not name in self.meters:\n",
    "            self.meters[name] = AverageMeter()\n",
    "        self.meters[name].update(value, n)\n",
    "\n",
    "    def reset(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.reset()\n",
    "\n",
    "    def values(self, postfix=''):\n",
    "        return {name + postfix: meter.val for name, meter in self.meters.items()}\n",
    "\n",
    "    def averages(self, postfix='/avg'):\n",
    "        return {name + postfix: meter.avg for name, meter in self.meters.items()}\n",
    "\n",
    "    def sums(self, postfix='/sum'):\n",
    "        return {name + postfix: meter.sum for name, meter in self.meters.items()}\n",
    "\n",
    "    def counts(self, postfix='/count'):\n",
    "        return {name + postfix: meter.count for name, meter in self.meters.items()}\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __format__(self, format):\n",
    "        return \"{self.val:{format}} ({self.avg:{format}})\".format(self=self, format=format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import HoromaDataset\n",
    "\n",
    "from comet_ml import OfflineExperiment\n",
    "import json\n",
    "import argparse\n",
    "from models import *\n",
    "from models.clustering import *\n",
    "from utils.ali_utils import *\n",
    "from utils.utils import *\n",
    "from utils.utils import load_datasets\n",
    "from utils.constants import Constants\n",
    "from data.dataset import HoromaDataset\n",
    "import torch\n",
    "path_to_model = None\n",
    "config_key = 'HALI'\n",
    "config = 'HALI'\n",
    "\n",
    "with open(Constants.CONFIG_PATH, 'r') as f:\n",
    "    configuration = json.load(f)[config_key]\n",
    "\n",
    "# Parse configuration file\n",
    "clustering_model = configuration['cluster_model']\n",
    "encoding_model = configuration['enc_model']\n",
    "batch_size = configuration['batch_size']\n",
    "seed = configuration['seed']\n",
    "n_epochs = configuration['n_epochs']\n",
    "train_subset = configuration['train_subset']\n",
    "train_split = configuration['train_split']\n",
    "valid_split = configuration['valid_split']\n",
    "train_labeled_split = configuration['train_labeled_split']\n",
    "encode = configuration['encode']\n",
    "cluster = configuration['cluster']\n",
    "flattened = False  # Default\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set all seeds for full reproducibility\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "datapath = Constants.DATAPATH\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "train = HoromaDataset(datapath, split=train_split, subset=train_subset,\n",
    "                      flattened=flattened)\n",
    "labeled = HoromaDataset(datapath, split=train_labeled_split, subset=train_subset,\n",
    "                        flattened=flattened)\n",
    "valid_data = HoromaDataset(\n",
    "    datapath, split=valid_split, subset=train_subset, flattened=flattened)\n",
    "\n",
    "train_label_indices = labeled.targets\n",
    "valid_indices = valid_data.targets\n",
    "\n",
    "print(\"Shape of training set: \", train.data.shape)\n",
    "print(\"Shape of validation set: \", labeled.data.shape)\n",
    "print(\"Shape of validation set: \", valid_data.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=opt.batch_size, shuffle=True)\n",
    "labeled_loader = DataLoader(labeled, batch_size=opt.labeled_batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(valid_data, batch_size=opt.labeled_batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = np.floor(labeled.data.shape[0]/opt.labeled_batch_size)\n",
    "print(n_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = th.optim.Adam(net.parameters(), opt.lr[0], weight_decay=opt.weight_decay)\n",
    "\n",
    "prev_time = datetime.datetime.now()\n",
    "best_valid_acc = 0\n",
    "iter_indx = 0\n",
    "\n",
    "epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsup_batch = next(iter(train_loader))\n",
    "sup_batch,target = next(iter(labeled_loader))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsup_batch.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up unlabeled input and labeled input with the corresponding labels\n",
    "input_unsup_var = th.autograd.Variable(unsup_batch[0:(opt.batch_size - opt.labeled_batch_size)]).to(device)\n",
    "input_sup_var = th.autograd.Variable(sup_batch).to(device)\n",
    "target_sup_var = th.autograd.Variable(target.data.long()).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_unsup_size = opt.batch_size\n",
    "minibatch_sup_size = opt.labeled_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute loss for unlabeled input\n",
    "[output_unsup, xhat_unsup, loss_pn_unsup, loss_bnmm_unsup] = net(input_unsup_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_reconst_unsup = L2_loss(xhat_unsup, input_unsup_var).mean()\n",
    "softmax_unsup = F.softmax(output_unsup)\n",
    "loss_kl_unsup = -th.sum(th.log(10.0*softmax_unsup + 1e-8) * softmax_unsup) / minibatch_unsup_size\n",
    "loss_unsup = opt.alpha_reconst * loss_reconst_unsup + opt.alpha_kl * loss_kl_unsup + opt.alpha_bnmm * loss_bnmm_unsup + opt.alpha_pn * loss_pn_unsup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute loss for labeled input\n",
    "[output_sup, xhat_sup, loss_pn_sup, loss_bnmm_sup] = net(input_sup_var, target_sup_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_xentropy_sup = criterion(output_sup, target_sup_var.squeeze_()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_xentropy_sup = criterion(output_sup, target_sup_var.squeeze_()) / minibatch_sup_size\n",
    "loss_reconst_sup = L2_loss(xhat_sup, input_sup_var).mean()\n",
    "softmax_sup = F.softmax(output_sup)\n",
    "loss_kl_sup = -th.sum(th.log(10.0*softmax_sup + 1e-8) * softmax_sup)/ minibatch_sup_size\n",
    "loss_sup = loss_xentropy_sup + opt.alpha_reconst * loss_reconst_sup + opt.alpha_kl * loss_kl_sup + opt.alpha_bnmm * loss_bnmm_sup + opt.alpha_pn * loss_pn_sup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_accuracies = []\n",
    "f1_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training loop\n",
    "# def train(net, train_loader,labeled_loader, eval_loader, num_epochs, wd):\n",
    "\n",
    "wd = 5e-4\n",
    "num_epochs = 2000\n",
    "\n",
    "\n",
    "\n",
    "best_f1 = 0\n",
    "\n",
    "\n",
    "for epoch in range(1,num_epochs):\n",
    "    train_loss = 0; train_loss_xentropy = 0; train_loss_reconst = 0; train_loss_pn = 0; train_loss_kl = 0; train_loss_bnmm = 0\n",
    "    correct = 0\n",
    "    num_batch_train = 0\n",
    "\n",
    "    # start with adam optimizer but switch sgd optimizer with exponential decay learning rate since epoch 20 \n",
    "    if epoch == 200:\n",
    "        sgd_lr = opt.lr[1]\n",
    "        decay_val = np.exp(np.log(sgd_lr / 0.0001) / (num_epochs - 2))\n",
    "        sgd_lr = sgd_lr * decay_val\n",
    "        trainer = th.optim.SGD(net.parameters(), sgd_lr, weight_decay=wd)\n",
    "\n",
    "    if epoch >= 150:\n",
    "        for param_group in trainer.param_groups:\n",
    "            param_group['lr'] = param_group['lr']/decay_val\n",
    "\n",
    "    for param_group in trainer.param_groups:\n",
    "        learning_rate = param_group['lr']\n",
    "\n",
    "    meters = AverageMeterSet()\n",
    "\n",
    "    # switch to train mode\n",
    "    net.train()\n",
    "\n",
    "#     end = time.time()\n",
    "    for i in range(int(n_iterations)):\n",
    "#         meters.update('data_time', time.time() - end)\n",
    "\n",
    "        unsup_batch = next(iter(train_loader))\n",
    "        sup_batch,target = next(iter(labeled_loader))\n",
    "\n",
    "        # set up unlabeled input and labeled input with the corresponding labels\n",
    "        input_unsup_var = th.autograd.Variable(unsup_batch[0:(opt.batch_size - opt.labeled_batch_size)]).to(device)\n",
    "        input_sup_var = th.autograd.Variable(sup_batch).to(device)\n",
    "        target_sup_var = th.autograd.Variable(target.data.long()).to(device)\n",
    "\n",
    "        minibatch_unsup_size = opt.batch_size - opt.labeled_batch_size\n",
    "        minibatch_sup_size = opt.labeled_batch_size\n",
    "\n",
    "        # compute loss for unlabeled input\n",
    "        [output_unsup, xhat_unsup, loss_pn_unsup, loss_bnmm_unsup] = net(input_unsup_var)\n",
    "        loss_reconst_unsup = L2_loss(xhat_unsup, input_unsup_var).mean()\n",
    "        softmax_unsup = F.softmax(output_unsup)\n",
    "        loss_kl_unsup = -th.sum(th.log(10.0*softmax_unsup + 1e-8) * softmax_unsup) / minibatch_unsup_size\n",
    "        loss_unsup = opt.alpha_reconst * loss_reconst_unsup + opt.alpha_kl * loss_kl_unsup + opt.alpha_bnmm * loss_bnmm_unsup + opt.alpha_pn * loss_pn_unsup\n",
    "\n",
    "\n",
    "\n",
    "        # compute loss for labeled input\n",
    "        [output_sup, xhat_sup, loss_pn_sup, loss_bnmm_sup] = net(input_sup_var, target_sup_var.squeeze_())\n",
    "        loss_xentropy_sup = criterion(output_sup, target_sup_var) / minibatch_sup_size\n",
    "        loss_reconst_sup = L2_loss(xhat_sup, input_sup_var).mean()\n",
    "        softmax_sup = F.softmax(output_sup)\n",
    "        loss_kl_sup = -th.sum(th.log(10.0*softmax_sup + 1e-8) * softmax_sup)/ minibatch_sup_size\n",
    "        loss_sup = loss_xentropy_sup + opt.alpha_reconst * loss_reconst_sup + opt.alpha_kl * loss_kl_sup + opt.alpha_bnmm * loss_bnmm_sup + opt.alpha_pn * loss_pn_sup\n",
    "\n",
    "        loss = th.mean(loss_unsup + loss_sup)\n",
    "\n",
    "        # compute the grads and update the parameters\n",
    "        trainer.zero_grad()\n",
    "        loss.backward()\n",
    "        trainer.step()\n",
    "\n",
    "        # accumulate all the losses for visualization\n",
    "        loss_reconst = loss_reconst_unsup + loss_reconst_sup\n",
    "        loss_pn = loss_pn_unsup + loss_pn_sup\n",
    "        loss_xentropy = loss_xentropy_sup\n",
    "        loss_kl = loss_kl_unsup + loss_kl_sup\n",
    "        loss_bnmm = loss_bnmm_unsup + loss_bnmm_sup\n",
    "\n",
    "        train_loss_xentropy += th.mean(loss_xentropy).cpu().detach().numpy()\n",
    "        train_loss_reconst += th.mean(loss_reconst).cpu().detach().numpy()\n",
    "        train_loss_pn += th.mean(loss_pn).cpu().detach().numpy()\n",
    "        train_loss_kl += th.mean(loss_kl).cpu().detach().numpy()\n",
    "        train_loss_bnmm += th.mean(loss_bnmm).cpu().detach().numpy()\n",
    "        train_loss += th.mean(loss).cpu().detach().numpy()\n",
    "        correct += get_acc(output_sup, target_sup_var).cpu().detach().numpy()\n",
    "\n",
    "        num_batch_train += 1\n",
    "        iter_indx += 1\n",
    "    writer.add_scalars('loss', {'train': train_loss / num_batch_train}, epoch)\n",
    "    writer.add_scalars('loss_xentropy', {'train': train_loss_xentropy / num_batch_train}, epoch)\n",
    "    writer.add_scalars('loss_reconst', {'train': train_loss_reconst / num_batch_train}, epoch)\n",
    "    writer.add_scalars('loss_pn', {'train': train_loss_pn / num_batch_train}, epoch)\n",
    "    writer.add_scalars('loss_kl', {'train': train_loss_kl / num_batch_train}, epoch)\n",
    "    writer.add_scalars('loss_bnmm', {'train': train_loss_bnmm / num_batch_train}, epoch)\n",
    "    writer.add_scalars('acc', {'train': correct / num_batch_train}, epoch)\n",
    "\n",
    "    cur_time = datetime.datetime.now()\n",
    "    h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "    m, s = divmod(remainder, 60)\n",
    "    time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "\n",
    "    # Validation\n",
    "    valid_loss = 0; valid_loss_xentropy = 0; valid_loss_reconst = 0; valid_loss_pn = 0; valid_loss_kl = 0; valid_loss_bnmm = 0\n",
    "    valid_correct = 0\n",
    "    num_batch_valid = 0\n",
    "    valid_accuracy = 0\n",
    "    valid_f1 = 0\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    for i, (batch, target) in enumerate(eval_loader):\n",
    "        with th.no_grad():\n",
    "            input_var = th.autograd.Variable(batch).to(device)\n",
    "            target_var = th.autograd.Variable(target.data.long()).to(device)\n",
    "\n",
    "            minibatch_size = len(target_var)\n",
    "\n",
    "            [output, xhat, loss_pn, loss_bnmm] = net(input_var, target_var)\n",
    "\n",
    "            loss_xentropy = criterion(output, target_var.squeeze_())/minibatch_size\n",
    "            loss_reconst = L2_loss(xhat, input_var).mean()\n",
    "            softmax_val = F.softmax(output)\n",
    "            loss_kl = -th.sum(th.log(10.0*softmax_val + 1e-8) * softmax_val)/minibatch_size\n",
    "            loss = loss_xentropy + opt.alpha_reconst * loss_reconst + opt.alpha_kl * loss_kl + opt.alpha_bnmm * loss_bnmm + opt.alpha_pn * loss_pn\n",
    "\n",
    "            valid_loss_xentropy += th.mean(loss_xentropy).cpu().detach().numpy()\n",
    "            valid_loss_reconst += th.mean(loss_reconst).cpu().detach().numpy()\n",
    "            valid_loss_pn += th.mean(loss_pn).cpu().detach().numpy()\n",
    "            valid_loss_kl += th.mean(loss_kl).cpu().detach().numpy()\n",
    "            valid_loss_bnmm += th.mean(loss_bnmm).cpu().detach().numpy()\n",
    "            valid_loss += th.mean(loss).cpu().detach().numpy()\n",
    "            valid_correct += get_acc(output, target_var).cpu().detach().numpy()\n",
    "            \n",
    "            accuracy, f1 = compute_metrics(target_var.cpu(), th.argmax(output, dim=1, keepdim=False).cpu())\n",
    "            valid_accuracy+=accuracy\n",
    "            valid_f1+=f1\n",
    "            num_batch_valid += 1\n",
    "    \n",
    "    valid_accuracies.append(valid_accuracy/num_batch_valid)\n",
    "    f1_scores.append(valid_f1/num_batch_valid)\n",
    "    valid_acc = valid_correct / num_batch_valid\n",
    "    f1_s = valid_f1/num_batch_valid\n",
    "    if f1_s > best_f1:\n",
    "        best_f1 = f1_s\n",
    "        th.save(net.state_dict(), '%s/%s_best_%i.pth'%(opt.model_dir, opt.exp_name, 1))\n",
    "#         th.save(net.state_dict(), '%s/%s_best.pth'%(opt.model_dir, opt.exp_name))\n",
    "#     writer.add_scalars('loss', {'valid': valid_loss / num_batch_valid}, epoch)\n",
    "#     writer.add_scalars('loss_xentropy', {'valid': valid_loss_xentropy / num_batch_valid}, epoch)\n",
    "#     writer.add_scalars('loss_reconst', {'valid': valid_loss_reconst / num_batch_valid}, epoch)\n",
    "#     writer.add_scalars('loss_pn', {'valid': valid_loss_pn / num_batch_valid}, epoch)\n",
    "#     writer.add_scalars('loss_kl', {'valid': valid_loss_kl / num_batch_valid}, epoch)\n",
    "#     writer.add_scalars('loss_bnmm', {'valid': valid_loss_bnmm / num_batch_valid}, epoch)\n",
    "#     writer.add_scalars('acc', {'valid': valid_acc}, epoch)\n",
    "    epoch_str = (\"Epoch %d. Train Loss: %f, Train Xent: %f, Train Reconst: %f, Train Pn: %f, Train acc %f, Valid Loss: %f, Valid acc %f, Best f1 acc %f,f1 %f, acc %f \"\n",
    "                 % (epoch, train_loss / num_batch_train, train_loss_xentropy / num_batch_train, train_loss_reconst / num_batch_train, train_loss_pn / num_batch_train,\n",
    "                    correct / num_batch_train, valid_loss / num_batch_valid, valid_acc, best_f1,valid_f1/num_batch_valid,valid_accuracy/num_batch_valid))\n",
    "#     if not epoch % 20:\n",
    "#         th.save(net.state_dict(), '%s/%s_epoch_%i.pth'%(opt.model_dir, opt.exp_name, epoch))\n",
    "\n",
    "    prev_time = cur_time\n",
    "#     logging.info(epoch_str + time_str + ', lr ' + str(learning_rate))\n",
    "    print(epoch_str)   \n",
    "#     return best_valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from utils.constants import Constants\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(('(Best_acc: %f. Best F1: %f, Best_model: %f, Exp Best f1: %f, Exp Best Model: f%'%(best_acc,best_f1,best_model,exp_best_f1,exp_best_model)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(f1_scores,label='F1 Score')\n",
    "ax.plot(valid_accuracies, label='Accuracy')\n",
    "ax.legend(loc='best')\n",
    "plt.title('Neural Rendering Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        th.nn.init.xavier_uniform(m.weight)\n",
    "        # m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Bias') != -1:\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "#     acc = train(model, train_loader, eval_loader, opt.num_epochs, opt.weight_decay)\n",
    "#     logging.info('Validation Accuracy - Run %i = %f'%(i, acc))\n",
    "#     valid_acc += acc\n",
    "\n",
    "#     logging.info('Validation Accuracy = %f'%(valid_acc/num_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "\n",
    "cfg = {\n",
    "    'AllConv13': [128, 128, 128, 'M', 256, 256, 256, 'M', 512, 256, 128, 'A'],\n",
    "}\n",
    "\n",
    "#################### Some utils class ####################\n",
    "class Reshape(nn.Module):\n",
    "    \"\"\"\n",
    "    Flatten the output of the convolutional layer\n",
    "    Parameters\n",
    "    ----------\n",
    "    Input shape: (N, C * W * H)\n",
    "    Output shape: (N, C, W, H)\n",
    "    \"\"\"\n",
    "    def __init__(self, shape, **kwargs):\n",
    "        super(Reshape, self).__init__(**kwargs)\n",
    "        self._shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.reshape(x.size()[0], self._shape[0], self._shape[1], self._shape[2])\n",
    "    \n",
    "class BiasAdder(nn.Module):\n",
    "    \"\"\"\n",
    "    Add a bias into the input\n",
    "    \"\"\"\n",
    "    def __init__ (self, channels, **kwargs):\n",
    "        super(BiasAdder, self).__init__(**kwargs)\n",
    "        self.bias = nn.Parameter(th.Tensor(1,channels,1,1))\n",
    "        self.bias.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.bias\n",
    "    \n",
    "class Flatten(nn.Module):\n",
    "    \"\"\"\n",
    "    Flatten 4D tensor into 2D tensor\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "    \n",
    "class Upaverage(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsample to reverse the avg pooling layer\n",
    "    \"\"\"\n",
    "    def __init__(self, scale_factor, **kwargs):\n",
    "        super(Upaverage, self).__init__(**kwargs)\n",
    "        self.scale_factor = scale_factor\n",
    "        self.upsample_layer = nn.Upsample(scale_factor=self.scale_factor, mode='nearest')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.upsample_layer(x) * (1./self.scale_factor)**2\n",
    "    \n",
    "def make_one_hot(labels, C=2):\n",
    "    \"\"\"\n",
    "    Converts an integer label torch.autograd.Variable to a one-hot Variable.\n",
    "    \"\"\"\n",
    "    target = th.eye(C)[labels.data]\n",
    "    target = target.to(labels.get_device())      \n",
    "    return target\n",
    "\n",
    "#################### Main NRM class ####################\n",
    "class NRM(nn.Module):\n",
    "    def __init__(self, net_name, batch_size, num_class, use_bias=False, use_bn=False, do_topdown=False, do_pn=False, do_bnmm=False):\n",
    "        super(NRM, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.do_topdown = do_topdown\n",
    "        self.do_pn = do_pn\n",
    "        self.do_bnmm = do_bnmm\n",
    "        self.use_bn = use_bn\n",
    "        self.use_bias = use_bias\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # create:\n",
    "        # feature extractor in the forward cnn step: self.features\n",
    "        # corresponding layer inm the top-down reconstruction nrm step: layers_nrm\n",
    "        # instance norm used in the top-down reconstruction nrm step: insnorms_nrm\n",
    "        # instance norm used in the forward cnn step: insnorms_cnn\n",
    "        self.features, layers_nrm, insnorms_nrm, insnorms_cnn = self._make_layers(cfg[net_name], use_bias, use_bn, self.do_topdown)\n",
    "        \n",
    "        # create the classifer in the forward cnn step\n",
    "        conv_layer = nn.Conv2d(in_channels=cfg[net_name][-2], out_channels=self.num_class, kernel_size=(1,1), bias=True)\n",
    "        flatten_layer = Flatten()\n",
    "        self.classifier = nn.Sequential(OrderedDict([('conv',conv_layer), ('flatten', flatten_layer)]))\n",
    "        \n",
    "        # create the nrm\n",
    "        if self.do_topdown:\n",
    "            # add layers corresponding to the classifer in the forward step\n",
    "            convtd_layer = nn.ConvTranspose2d(out_channels=cfg[net_name][-2], in_channels=self.num_class, kernel_size=(1,1), stride=(1, 1), bias=False)\n",
    "            convtd_layer.weight.data = conv_layer.weight.data\n",
    "            layers_nrm += [('convtd',convtd_layer), ('reshape', Reshape(shape=(self.num_class, 1, 1)))]\n",
    "            \n",
    "            self.nrm = nn.Sequential(OrderedDict(layers_nrm[::-1]))\n",
    "            \n",
    "            # if use path normalization, then also use instance normalization\n",
    "            if self.do_pn:\n",
    "                self.insnorms_nrm = nn.Sequential(OrderedDict(insnorms_nrm[::-1]))\n",
    "                self.insnorms_cnn = nn.Sequential(OrderedDict(insnorms_cnn))\n",
    "\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        ahat = []; that = []; bcnn = []; apn = []; meancnn = []; varcnn = []\n",
    "        xbias = th.zeros([1, x.shape[1], x.shape[2], x.shape[3]], device=x.get_device()) if self.do_pn else []\n",
    "        insnormcnn_indx = 0\n",
    "        \n",
    "        # if do top-down reconstruction, we need to keep track of relu state, maxpool state,\n",
    "        # mean and var of the activations, and the bias terms in the forward cnn step\n",
    "        if self.do_topdown: \n",
    "            for name, layer in self.features.named_children():\n",
    "                if name.find('pool') != -1 and not name.find('average') != -1: # keep track of the maxpool state\n",
    "                    F.interpolate(layer(x), scale_factor=2, mode='nearest')\n",
    "                    that.append(th.gt(x-F.interpolate(layer(x), scale_factor=2, mode='nearest'),0))\n",
    "                    x = layer(x)\n",
    "                    if self.do_pn:\n",
    "                        xbias = layer(xbias)\n",
    "                else:\n",
    "                    x = layer(x)\n",
    "                    \n",
    "                    if self.do_pn: # get the forward results to compute the path normalization later\n",
    "                        if name.find('batchnorm') != -1:\n",
    "                            xbias = self.insnorms_cnn[insnormcnn_indx](xbias)\n",
    "                            insnormcnn_indx += 1\n",
    "                        else:\n",
    "                            xbias = layer(xbias)\n",
    "                    if name.find('relu') != -1: # keep track of the relu state\n",
    "                        ahat.append(th.gt(x,0) + th.le(x,0)*0.1)\n",
    "                        if self.do_pn:\n",
    "                            apn.append(th.gt(xbias,0) + th.le(xbias,0)*0.1)\n",
    "                    \n",
    "                    if self.use_bn:\n",
    "                        if name.find('conv') != -1: # keep track of the mean and var of the activations\n",
    "                            meancnn.append(th.mean(x, dim=(0,2,3), keepdim=True))\n",
    "                            varcnn.append(th.mean((x - th.mean(x, dim=(0,2,3), keepdim=True))**2, dim=(0,2,3), keepdim=True))\n",
    "                        if self.use_bias: # keep track of the bias terms when adding bias\n",
    "                            if name.find('bias') != -1: \n",
    "                                bcnn.append(layer.bias)\n",
    "                        else: # otherwise, keep track of the bias terms inside the batch norm\n",
    "                            if name.find('batchnorm') != -1:\n",
    "                                bcnn.append(layer.bias)\n",
    "                    else:\n",
    "                        if self.use_bias:\n",
    "                            if name.find('conv') != -1:\n",
    "                                bcnn.append(layer.bias)\n",
    "            \n",
    "            # reverse the order of the parameters/variables that we keep track to use in the top-down reconstruction nrm step since nrm is the reverse of cnn\n",
    "            ahat = ahat[::-1]\n",
    "            that = that[::-1]\n",
    "            bcnn = bcnn[::-1]\n",
    "            apn = apn[::-1]\n",
    "            meancnn = meancnn[::-1]\n",
    "            varcnn = varcnn[::-1]\n",
    "        else:\n",
    "            x =  self.features(x)\n",
    "        \n",
    "        # send the features into the classifier\n",
    "        z = self.classifier(x)\n",
    "        \n",
    "        # do reconstruction via nrm\n",
    "        # xhat: the reconstruction image\n",
    "        # loss_pn: path normalization loss\n",
    "        # loss_bnmm: batch norm moment matching loss\n",
    "        if self.do_topdown:\n",
    "            xhat, _, loss_pn, loss_bnmm = self.topdown(self.nrm, make_one_hot(y, self.num_class), ahat, that, bcnn, th.ones([1, z.size()[1]], device=z.get_device()), apn, meancnn, varcnn) if y is not None else self.topdown(self.nrm, make_one_hot(th.argmax(z.detach(), dim=1), self.num_class), ahat, that, bcnn, th.ones([1, z.size()[1]], device=z.get_device()), apn, meancnn, varcnn)\n",
    "        else:\n",
    "            xhat = None\n",
    "            loss_pn = None\n",
    "            loss_bnmm = None\n",
    "\n",
    "\n",
    "        return [z, xhat, loss_pn, loss_bnmm]\n",
    "\n",
    "    def _make_layers(self, cfg, use_bias, use_bn, do_topdown):\n",
    "        layers = []\n",
    "        layers_nrm = []\n",
    "        insnorms_nrm = []\n",
    "        insnorms_cnn = []\n",
    "        in_channels = 3\n",
    "\n",
    "        for i, x in enumerate(cfg):\n",
    "            if x == 'M': # if max pooling layer, then add max pooling and dropout into the cnn. Add upsample layers, dropout, batchnorm, and instance norm - for path normaliztion - into the nrm.\n",
    "                layers += [('pool%i'%i, nn.MaxPool2d(2, stride=2)), ('dropout%i'%i, nn.Dropout(0.5))]\n",
    "                if do_topdown:\n",
    "                    if use_bn:\n",
    "                        layers_nrm += [('upsample%i'%i, nn.Upsample(scale_factor=2, mode='nearest')), ('dropout%i'%i, nn.Dropout(0.5)), ('batchnorm%i'%i, nn.BatchNorm2d(cfg[i-1]))]\n",
    "                        insnorms_nrm += [('instancenormtd%i'%i, nn.InstanceNorm2d(cfg[i-1], affine=True))]\n",
    "                    else:\n",
    "                        layers_nrm += [('upsample%i'%i, nn.Upsample(scale_factor=2, mode='nearest')), ('dropout%i'%i, nn.Dropout(0.5))]\n",
    "                        \n",
    "            elif x == 'A': # if avg pooling layer, then add average pooling layer into the cnn. Add up average layers, batchnorm and instance norm - for path normaliztion - into the nrm.\n",
    "                layers += [('average%i'%i, nn.AvgPool2d(6, stride=1))]\n",
    "                if do_topdown:\n",
    "                    if use_bn:\n",
    "                        layers_nrm += [('upaverage%i'%i, Upaverage(scale_factor=6)), ('batchnorm%i'%i, nn.BatchNorm2d(cfg[i-1]))]\n",
    "                        insnorms_nrm += [('instancenormtd%i'%i, nn.InstanceNorm2d(cfg[i-1], affine=True))]\n",
    "                    else:\n",
    "                        layers_nrm += [('upaverage%i'%i, Upaverage(scale_factor=6))]\n",
    "                        \n",
    "            else: # add other layers into the cnn and the nrm\n",
    "                padding_cnn = (0,0) if x == 512 else (1,1)\n",
    "                padding_nrm = (0,0) if x == 512 else (1,1)\n",
    "                if use_bn:\n",
    "                    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=x, kernel_size=(3,3), padding=padding_cnn, bias=False)\n",
    "                    if use_bias:\n",
    "                        layers += [('conv%i'%i, conv_layer),\n",
    "                                   ('batchnorm%i'%i, nn.BatchNorm2d(x)),\n",
    "                                   ('bias%i'%i,BiasAdder(channels=x)),\n",
    "                                   ('relu%i'%i,nn.LeakyReLU(0.1))]\n",
    "                    else:\n",
    "                        layers += [('conv%i'%i, conv_layer),\n",
    "                                   ('batchnorm%i'%i, nn.BatchNorm2d(x)),\n",
    "                                   ('relu%i'%i, nn.LeakyReLU(0.1))]\n",
    "                    \n",
    "                    insnorms_cnn += [('instancenormcnn%i'%i, nn.InstanceNorm2d(x, affine=True))]\n",
    "                    if do_topdown:\n",
    "                        if (cfg[i-1] == 'M' or cfg[i-1] == 'A') and not i == 0:\n",
    "                            layers_nrm += [('convtd%i'%i, nn.ConvTranspose2d(out_channels=in_channels, in_channels=x, kernel_size=3, stride=(1, 1),\n",
    "                                                              padding=padding_nrm, bias=False))]\n",
    "                            layers_nrm[-1][-1].weight.data = conv_layer.weight.data\n",
    "                        else:\n",
    "                            layers_nrm += [('batchnormtd%i'%i, nn.BatchNorm2d(in_channels)), ('convtd%i'%i, nn.ConvTranspose2d(out_channels=in_channels, in_channels=x, kernel_size=3, stride=(1, 1), padding=padding_nrm, bias=False))]\n",
    "                            layers_nrm[-1][-1].weight.data = conv_layer.weight.data\n",
    "                            insnorms_nrm += [('instancenormtd%i'%i, nn.InstanceNorm2d(in_channels, affine=True))]\n",
    "                    \n",
    "                elif use_bias:\n",
    "                    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=x, kernel_size=(3,3), padding=padding_cnn, use_bias=True)\n",
    "                    layers += [('conv%i'%i, conv_layer), ('relu%i'%i, nn.LeakyReLU(0.1))]\n",
    "                    if do_topdown:\n",
    "                        layers_nrm += [('convtd%i'%i, nn.ConvTranspose2d(out_channels=in_channels, in_channels=x, kernel_size=3, stride=(1, 1),\n",
    "                                                          padding=padding_nrm, bias=False))]\n",
    "                        layers_nrm[-1][-1].weight.data = conv_layer.weight.data\n",
    "                    \n",
    "                else:\n",
    "                    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=x, kernel_size=(3,3), padding=padding_cnn, bias=False)\n",
    "                    layers += [('conv%i'%i, conv_layer), ('relu%i'%i, nn.LeakyReLU(0.1))]\n",
    "                    if do_topdown:\n",
    "                        layers_nrm += [('convtd%i'%i, nn.ConvTranspose2d(out_channels=in_channels, in_channels=x, kernel_size=3, stride=(1,1),\n",
    "                                                          padding=padding_nrm, bias=False))]\n",
    "                        layers_nrm[-1][-1].weight.data = conv_layer.weight.data\n",
    "                        \n",
    "                in_channels = x\n",
    "\n",
    "        model = nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "        return model, layers_nrm, insnorms_nrm, insnorms_cnn\n",
    "\n",
    "    def topdown(self, net, xhat, ahat, that, bcnn, xpn, apn, meancnn, varcnn):\n",
    "        mu = xhat\n",
    "        mupn = xpn\n",
    "        loss_pn = th.zeros([self.batch_size,], device=mu.get_device())\n",
    "        loss_bnmm = th.zeros([self.batch_size,], device=mu.get_device())\n",
    "\n",
    "        ahat_indx = 0; that_indx = 0; meanvar_indx = 0; insnormtd_indx = 0\n",
    "        prev_name = ''\n",
    "        \n",
    "        for i, (name, layer) in enumerate(net.named_children()):\n",
    "            if name.find('conv') != -1 and i > 1: \n",
    "                mu = mu * ahat[ahat_indx].type(th.FloatTensor).to(mu.get_device()) # mask the intermediate rendered images by the relu states in the forward step\n",
    "                \n",
    "                if self.do_pn: # compute the path normalization loss\n",
    "                    mupn = mupn * apn[ahat_indx].type(th.FloatTensor).to(mu.get_device())\n",
    "                    mu_b = bcnn[ahat_indx].data.reshape((1, -1, 1, 1)) * mu\n",
    "                    mupn_b = bcnn[ahat_indx].data.reshape((1, -1, 1, 1)) * mupn\n",
    "                    \n",
    "                    loss_pn_layer = th.mean(th.abs(mu_b - mupn_b), dim=(1,2,3))\n",
    "                    loss_pn = loss_pn + loss_pn_layer\n",
    "\n",
    "                ahat_indx += 1\n",
    "\n",
    "            if prev_name.find('upsamplelayer') != -1 and not prev_name.find('avg') != -1:\n",
    "                mu = mu * that[that_indx].type(th.FloatTensor).to(mu.get_device()) # mask the intermediate rendered images by the maxpool states in the forward step\n",
    "                if self.do_pn:\n",
    "                    mupn = mupn * that[that_indx].type(th.FloatTensor).to(mu.get_device())\n",
    "                that_indx += 1\n",
    "          \n",
    "            # compute the next intermediate rendered images\n",
    "            mu = layer(mu)\n",
    "            \n",
    "            # compute the next intermediate rendered results for computing the path normalization loss in the next layer\n",
    "            if (name.find('batchnorm') != -1) and (i < len(net) - 1):\n",
    "                if self.do_pn:\n",
    "                    mupn = self.insnorms_nrm[insnormtd_indx](mupn)\n",
    "                    insnormtd_indx += 1\n",
    "            else:\n",
    "                if self.do_pn:\n",
    "                    mupn = layer(mupn)\n",
    "            \n",
    "            if (name.find('conv') != -1) and (i != (len(net)-2)):\n",
    "                if self.do_bnmm and self.use_bn:\n",
    "                    # compute the KL distance between two Gaussians - the intermediate rendered images and the mean/var from the forward step\n",
    "                    loss_bnmm = loss_bnmm + 0.5*th.mean(((th.mean(mu, dim=(0,2,3)) - meancnn[meanvar_indx])**2)/varcnn[meanvar_indx]) + 0.5*th.mean(th.mean((mu - th.mean(mu, dim=(0,2,3), keepdim=True))**2, dim=(0,2,3))/varcnn[meanvar_indx]) - 0.5*th.mean(th.log(th.mean((mu - th.mean(mu, dim=(0,2,3), keepdim=True))**2, dim=(0,2,3)) + 1e-8) - th.log(varcnn[meanvar_indx])) - 0.5\n",
    "                    meanvar_indx += 1\n",
    "                    \n",
    "            prev_name = name\n",
    "            \n",
    "        return mu, mupn, loss_pn, loss_bnmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
