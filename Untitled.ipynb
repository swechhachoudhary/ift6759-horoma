{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import OfflineExperiment\n",
    "import json\n",
    "import argparse\n",
    "from models import *\n",
    "from models.clustering import *\n",
    "from utils.ali_utils import *\n",
    "from utils.utils import *\n",
    "from utils.utils import load_datasets\n",
    "from utils.constants import Constants\n",
    "from data.dataset import HoromaDataset\n",
    "import torch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "datapath = Constants.DATAPATH\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "path_to_model = None\n",
    "config_key = 'HALI'\n",
    "config = 'HALI'\n",
    "\n",
    "with open(Constants.CONFIG_PATH, 'r') as f:\n",
    "    configuration = json.load(f)[config_key]\n",
    "\n",
    "# Parse configuration file\n",
    "clustering_model = configuration['cluster_model']\n",
    "encoding_model = configuration['enc_model']\n",
    "batch_size = configuration['batch_size']\n",
    "seed = configuration['seed']\n",
    "n_epochs = configuration['n_epochs']\n",
    "train_subset = configuration['train_subset']\n",
    "train_split = configuration['train_split']\n",
    "valid_split = configuration['valid_split']\n",
    "train_labeled_split = configuration['train_labeled_split']\n",
    "encode = configuration['encode']\n",
    "cluster = configuration['cluster']\n",
    "flattened = False  # Default\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set all seeds for full reproducibility\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "latent_dim = configuration['Zdim']\n",
    "\n",
    "experiment = OfflineExperiment(project_name='general',\n",
    "                               workspace='timothynest',  # Replace this with appropriate comet workspace\n",
    "                               offline_directory=\"experiments\")\n",
    "experiment.set_name(\n",
    "    name=config + \"_dim={}_overlapped={}\".format(latent_dim, train_split))\n",
    "experiment.log_parameters(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set:  (152228, 3, 32, 32)\n",
      "Shape of validation set:  (252, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize necessary objects\n",
    "clustering_model = SVMClustering(seed)\n",
    "\n",
    "train = HoromaDataset(datapath, split=train_split, subset=train_subset,\n",
    "                      flattened=flattened)\n",
    "labeled = HoromaDataset(datapath, split=train_labeled_split, subset=train_subset,\n",
    "                        flattened=flattened)\n",
    "valid_data = HoromaDataset(\n",
    "    datapath, split=valid_split, subset=train_subset, flattened=flattened)\n",
    "\n",
    "train_label_indices = labeled.targets\n",
    "valid_indices = valid_data.targets\n",
    "\n",
    "print(\"Shape of training set: \", train.data.shape)\n",
    "print(\"Shape of validation set: \", valid_data.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6, 11, 11, 11, 11, 16, 16,\n",
       "       16, 16, 16, 16, 16, 16, 16, 16,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "        5,  5,  5,  5,  5,  5,  5, 13, 13, 13, 13, 13, 13, 13, 13,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "       15, 15,  9,  9,  9,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  4,  4,  4,\n",
       "        4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "        4,  4,  4,  4,  7,  7,  7,  7, 12, 12, 12,  3,  3,  8])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(labeled, batch_size = 6,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,l = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if encode:\n",
    "    # Train and apply encoding model\n",
    "    if encoding_model ==\"hali\":\n",
    "        Gx1,Gx2,Gz1,Gz2,Disc,z_pred1,z_pred2,optim_g,optim_d,train_loader,cuda =  initialize_hali(configs,train)\n",
    "        training_loop_hali(Gz1,Gz2,Gx1,Gx2,Disc,optim_d,optim_g,train_loader,configs,experiment,cuda,z_pred1,z_pred2)\n",
    "    else: #default to ALI\n",
    "        Gx,Gz,Disc,z_pred,optim_g,optim_d,train_loader,cuda = initialize_ali(configs,train)\n",
    "        training_loop_ali(Gz,Gx,Disc,optim_d,optim_g,train_loader,configs,experiment,cuda,z_pred)                    \n",
    "else:\n",
    "    if encoding_model ==\"hali\":\n",
    "\n",
    "        Gx1,Gx2,Gz1,Gz2,Disc,z_pred1,z_pred2,optim_g,optim_d,train_loader,cuda =  initialize_hali(configs,train)\n",
    "        Gz1.load_state_dict(torch.load(configs['MODEL_PATH']+'/Gz1-'+str(configs['load_from_epoch'])+'.pth'))\n",
    "        Gz2.load_state_dict(torch.load(configs['MODEL_PATH']+'/Gz2-'+str(configs['load_from_epoch'])+'.pth'))\n",
    "        Gx1.load_state_dict(torch.load(configs['MODEL_PATH']+'/Gx1-'+str(configs['load_from_epoch'])+'.pth'))\n",
    "        Gx2.load_state_dict(torch.load(configs['MODEL_PATH']+'/Gx2-'+str(configs['load_from_epoch'])+'.pth'))\n",
    "\n",
    "    else: #default to ALI\n",
    "        Gx,Gz,Disc,z_pred,optim_g,optim_d,train_loader,cuda = initialize_ali(configs,train)\n",
    "        Gz.load_state_dict(torch.load(configs['MODEL_PATH']+'/Gz-'+str(configs['load_from_epoch'])+'.pth'))\n",
    "        Gx.load_state_dict(torch.load(configs['MODEL_PATH']+'/Gx-'+str(configs['load_from_epoch'])+'.pth'))\n",
    "\n",
    "if cluster:\n",
    "\n",
    "    if encoding_model ==\"hali\":\n",
    "        train_enc,train_labels = get_hali_embeddings(Gz1,Gz2,labeled[train_label_indices])\n",
    "        valid_enc,val_labels = get_hali_embeddings(Gz1,Gz2,labeled[valid_indices])\n",
    "    # else:\n",
    "\n",
    "    # Train and apply clustering model\n",
    "    clustering_model.train(train_enc)\n",
    "    cluster_labels = assign_labels_to_clusters(clustering_model, train_labeled_enc,\n",
    "                                               labeled.targets[train_label_indices])\n",
    "    _, accuracy, f1 = eval_model_predictions(clustering_model, valid_enc, labeled.targets[valid_indices],\n",
    "                                             cluster_labels)\n",
    "    experiment.log_metric('accuracy', accuracy)\n",
    "    experiment.log_metric('f1-score', f1)\n",
    "\n",
    "\n",
    "\n",
    "    # Save models\n",
    "    model = {'cluster': clustering_model,'cluster_labels': cluster_labels}\n",
    "    torch.save(model, configs['MODEL_PATH'] +\n",
    "               str(experiment.get_key()) + '.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
