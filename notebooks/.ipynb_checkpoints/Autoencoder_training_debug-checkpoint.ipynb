{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'configs.constants'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-679e40fb574b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHoromaTransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactories\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelFactory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptimizerFactory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainerFactory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/rap/jvb-000-aa/COURS2019/etudiants/user11/Block3/ift6759-horoma/utils/factories.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rap/jvb-000-aa/COURS2019/etudiants/user11/Block3/ift6759-horoma/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwasserstein\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWassersteinAutoEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msimple_ae\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleAutoEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvariational\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariationalAutoEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconv_ae\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvolutionalAutoEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rap/jvb-000-aa/COURS2019/etudiants/user11/Block3/ift6759-horoma/models/variational.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUpSample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mINPUT_CHANNELS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'configs.constants'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torchvision.transforms import functional\n",
    "from utils.transforms import HoromaTransforms\n",
    "from utils.factories import ModelFactory, OptimizerFactory, TrainerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config, resume, test_run=False, helios_run=None, horoma_test=False):\n",
    "    \"\"\"\n",
    "    Execute a training for a model.\n",
    "\n",
    "    :param config: the configuration of the optimizer, model and trainer.\n",
    "    :param resume: path to the checkpoint of a model.\n",
    "    :param test_run: whether it's a test run or not. In case of test run,\n",
    "    uses custom mnist dataset.\n",
    "    :param helios_run: start datetime of a run on helios.\n",
    "    :param horoma_test: whether to use the test horoma dataset or not.\n",
    "    \"\"\"\n",
    "    np.random.seed(config[\"numpy_seed\"])\n",
    "    torch.manual_seed(config[\"torch_seed\"])\n",
    "    torch.cuda.manual_seed_all(config[\"torch_seed\"])\n",
    "\n",
    "    # setup data_loader instances\n",
    "    if not test_run:\n",
    "        unlabelled = HoromaDataset(\n",
    "            **config[\"data\"][\"dataset\"],\n",
    "            split='train_overlapped',\n",
    "            transforms=HoromaTransforms()\n",
    "        )\n",
    "\n",
    "        labelled = HoromaDataset(\n",
    "            data_dir=config[\"data\"][\"dataset\"]['data_dir'],\n",
    "            flattened=False,\n",
    "            split='valid_overlapped',\n",
    "            transforms=HoromaTransforms()\n",
    "        )\n",
    "    elif horoma_test:\n",
    "\n",
    "        unlabelled = HoromaDataset(\n",
    "            **config[\"data\"][\"dataset\"],\n",
    "            split='train_overlapped',\n",
    "            transforms=HoromaTransforms(),\n",
    "            subset=5\n",
    "        )\n",
    "\n",
    "        labelled = HoromaDataset(\n",
    "            data_dir=config[\"data\"][\"dataset\"]['data_dir'],\n",
    "            flattened=False,\n",
    "            split='valid_overlapped',\n",
    "            transforms=HoromaTransforms(),\n",
    "            subset=5\n",
    "        )\n",
    "    else:\n",
    "        unlabelled = CustomMNIST(**config[\"data\"][\"dataset\"], subset=5000)\n",
    "        labelled = CustomLabelledMNIST(**config[\"data\"][\"dataset\"],\n",
    "                                       subset=1000)\n",
    "\n",
    "    model = ModelFactory.get(config)\n",
    "\n",
    "    print(model)\n",
    "    print()\n",
    "\n",
    "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = OptimizerFactory.get(config, trainable_params)\n",
    "\n",
    "    trainer = TrainerFactory.get(config)(\n",
    "        model,\n",
    "        optimizer,\n",
    "        resume=resume,\n",
    "        config=config,\n",
    "        unlabelled=unlabelled,\n",
    "        labelled=labelled,\n",
    "        helios_run=helios_run,\n",
    "        **config['trainer']['options']\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoromaDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir, split=\"train\", subset=None, skip=0,\n",
    "                 flattened=False, transforms=None):\n",
    "        \"\"\"\n",
    "        Initialize the horoma dataset.\n",
    "\n",
    "        :param data_dir: Path to the directory containing the samples.\n",
    "        :param split: Which split to use. [train, valid, test]\n",
    "        :param subset: Percentage size of dataset to use. Default: all.\n",
    "        :param skip: How many element to skip before taking the subset.\n",
    "        :param flattened: If True return the images in a flatten format.\n",
    "        :param transforms: Transforms to apply on the dataset before using it.\n",
    "        \"\"\"\n",
    "        nb_channels = 3\n",
    "        height = 32\n",
    "        width = 32\n",
    "        datatype = \"uint8\"\n",
    "\n",
    "        if split == \"train\":\n",
    "            #self.nb_examples = 150900\n",
    "            self.nb_examples = 10\n",
    "        elif split == \"valid\":\n",
    "            #self.nb_examples = 480\n",
    "            self.nb_examples = 10\n",
    "        elif split == \"test\":\n",
    "            #self.nb_examples = 498\n",
    "            self.nb_examples = 10\n",
    "        elif split == \"train_overlapped\":\n",
    "            #self.nb_examples = 544749\n",
    "            self.nb_examples = 10\n",
    "        elif split == \"valid_overlapped\":\n",
    "            #self.nb_examples = 1331\n",
    "            self.nb_examples = 10\n",
    "        else:\n",
    "            raise (\"Dataset: Invalid split. \"\n",
    "                   \"Must be [train, valid, test, train_overlapped, valid_overlapped]\")\n",
    "\n",
    "        filename_x = os.path.join(data_dir, \"{}_x.dat\".format(split))\n",
    "        filename_y = os.path.join(data_dir, \"{}_y.txt\".format(split))\n",
    "\n",
    "        filename_region_ids = os.path.join(data_dir,\n",
    "                                           \"{}_regions_id.txt\".format(split))\n",
    "        self.region_ids = np.loadtxt(filename_region_ids, dtype=object)\n",
    "\n",
    "        self.targets = None\n",
    "        if os.path.exists(filename_y) and not split.startswith(\"train\"):\n",
    "            pre_targets = np.loadtxt(filename_y, 'U2')\n",
    "\n",
    "            if subset is None:\n",
    "                pre_targets = pre_targets[skip: None]\n",
    "            else:\n",
    "                pre_targets = pre_targets[skip: skip + subset]\n",
    "\n",
    "            self.map_labels = np.unique(pre_targets)\n",
    "\n",
    "            self.targets = np.asarray([\n",
    "                np.where(self.map_labels == t)[0][0]\n",
    "                for t in pre_targets\n",
    "            ])\n",
    "        print(self.nb_examples)\n",
    "        print(height)\n",
    "        print(width)\n",
    "        print(nb_channels)\n",
    "        self.data = np.memmap(\n",
    "            filename_x,\n",
    "            dtype=datatype,\n",
    "            mode=\"r\",\n",
    "            shape=(self.nb_examples, height, width, nb_channels)\n",
    "        )\n",
    "\n",
    "        if subset is None:\n",
    "            self.data = self.data[skip: None]\n",
    "            self.region_ids = self.region_ids[skip: None]\n",
    "        else:\n",
    "            self.data = self.data[skip: skip + subset]\n",
    "            self.region_ids = self.region_ids[skip: skip + subset]\n",
    "\n",
    "        self.flattened = flattened\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index]\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        if self.flattened:\n",
    "            img = img.view(-1)\n",
    "\n",
    "        if self.targets is not None:\n",
    "            return img, torch.Tensor([self.targets[index]])\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'cnnautoencoder_cluster_kmeans_l20', 'n_gpu': 1, 'numpy_seed': 1, 'torch_seed': 1, 'wall_time': 8, 'model': {'type': 'ConvolutionalAutoEncoder', 'args': {'code_size': 20, 'dropout': 0.1, 'cnn1_out_channels': 10, 'cnn1_kernel_size': 5, 'cnn2_out_channels': 20, 'cnn2_kernel_size': 5, 'lin2_in_channels': 50, 'maxpool_kernel': 2, 'loss_fct': 'MSELoss'}}, 'optimizer': {'type': 'Adam', 'args': {'lr': 0.0001, 'weight_decay': 0, 'amsgrad': False}}, 'trainer': {'epochs': 100, 'save_dir': 'saved/', 'log_dir': 'logs/', 'save_period': 10, 'type': 'ClusterKMeansTrainer', 'options': {'n_clusters': 100, 'kmeans_interval': 0, 'kmeans_headstart': 0, 'kmeans_weight': 10.0}}, 'data': {'dataset': {'data_dir': '/rap/jvb-000-aa/COURS2019/etudiants/data/horoma', 'flattened': False}, 'dataloader': {'split': 0.9, 'train': {'batch_size': 128, 'shuffle': True}, 'valid': {'batch_size': 128, 'shuffle': False}}}}\n"
     ]
    }
   ],
   "source": [
    "config = json.load(open('../configs/cnnautoencoder_clusters_kmeans.json'))\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "32\n",
      "32\n",
      "3\n",
      "10\n",
      "32\n",
      "32\n",
      "3\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ModelFactory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-d0fbafe275ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run on a subsample of horoma dataset train_overlapped and valid_overlapped (only take the 5first sampless)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-f9f4ec10a537>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(config, resume, test_run, helios_run, horoma_test)\u001b[0m\n\u001b[1;32m     49\u001b[0m                                        subset=1000)\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelFactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ModelFactory' is not defined"
     ]
    }
   ],
   "source": [
    "# Run on a subsample of horoma dataset train_overlapped and valid_overlapped (only take the 5first sampless)\n",
    "main(config, None, True, None, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
