{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torchvision.transforms import functional\n",
    "from utils.transforms import HoromaTransforms\n",
    "from utils.factories import ModelFactory, OptimizerFactory, TrainerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config, resume, test_run=False, helios_run=None, horoma_test=False):\n",
    "    \"\"\"\n",
    "    Execute a training for a model.\n",
    "\n",
    "    :param config: the configuration of the optimizer, model and trainer.\n",
    "    :param resume: path to the checkpoint of a model.\n",
    "    :param test_run: whether it's a test run or not. In case of test run,\n",
    "    uses custom mnist dataset.\n",
    "    :param helios_run: start datetime of a run on helios.\n",
    "    :param horoma_test: whether to use the test horoma dataset or not.\n",
    "    \"\"\"\n",
    "    np.random.seed(config[\"numpy_seed\"])\n",
    "    torch.manual_seed(config[\"torch_seed\"])\n",
    "    torch.cuda.manual_seed_all(config[\"torch_seed\"])\n",
    "\n",
    "    # setup data_loader instances\n",
    "    if not test_run:\n",
    "        unlabelled = HoromaDataset(\n",
    "            **config[\"data\"][\"dataset\"],\n",
    "            split='train_overlapped',\n",
    "            transforms=HoromaTransforms()\n",
    "        )\n",
    "\n",
    "        labelled = HoromaDataset(\n",
    "            data_dir=config[\"data\"][\"dataset\"]['data_dir'],\n",
    "            flattened=False,\n",
    "            split='valid_overlapped',\n",
    "            transforms=HoromaTransforms()\n",
    "        )\n",
    "    elif horoma_test:\n",
    "\n",
    "        unlabelled = HoromaDataset(\n",
    "            **config[\"data\"][\"dataset\"],\n",
    "            split='train_overlapped',\n",
    "            transforms=HoromaTransforms(),\n",
    "            subset=5\n",
    "        )\n",
    "\n",
    "        labelled = HoromaDataset(\n",
    "            data_dir=config[\"data\"][\"dataset\"]['data_dir'],\n",
    "            flattened=False,\n",
    "            split='valid_overlapped',\n",
    "            transforms=HoromaTransforms(),\n",
    "            subset=5\n",
    "        )\n",
    "    else:\n",
    "        unlabelled = CustomMNIST(**config[\"data\"][\"dataset\"], subset=5000)\n",
    "        labelled = CustomLabelledMNIST(**config[\"data\"][\"dataset\"],\n",
    "                                       subset=1000)\n",
    "\n",
    "    model = ModelFactory.get(config)\n",
    "\n",
    "    print(model)\n",
    "    print()\n",
    "\n",
    "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = OptimizerFactory.get(config, trainable_params)\n",
    "\n",
    "    trainer = TrainerFactory.get(config)(\n",
    "        model,\n",
    "        optimizer,\n",
    "        resume=resume,\n",
    "        config=config,\n",
    "        unlabelled=unlabelled,\n",
    "        labelled=labelled,\n",
    "        helios_run=helios_run,\n",
    "        **config['trainer']['options']\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoromaDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir, split=\"train\", subset=None, skip=0,\n",
    "                 flattened=False, transforms=None):\n",
    "        \"\"\"\n",
    "        Initialize the horoma dataset.\n",
    "\n",
    "        :param data_dir: Path to the directory containing the samples.\n",
    "        :param split: Which split to use. [train, valid, test]\n",
    "        :param subset: Percentage size of dataset to use. Default: all.\n",
    "        :param skip: How many element to skip before taking the subset.\n",
    "        :param flattened: If True return the images in a flatten format.\n",
    "        :param transforms: Transforms to apply on the dataset before using it.\n",
    "        \"\"\"\n",
    "        nb_channels = 3\n",
    "        height = 32\n",
    "        width = 32\n",
    "        datatype = \"uint8\"\n",
    "\n",
    "        if split == \"train\":\n",
    "            #self.nb_examples = 150900\n",
    "            self.nb_examples = 10\n",
    "        elif split == \"valid\":\n",
    "            #self.nb_examples = 480\n",
    "            self.nb_examples = 10\n",
    "        elif split == \"test\":\n",
    "            #self.nb_examples = 498\n",
    "            self.nb_examples = 10\n",
    "        elif split == \"train_overlapped\":\n",
    "            #self.nb_examples = 544749\n",
    "            self.nb_examples = 10\n",
    "        elif split == \"valid_overlapped\":\n",
    "            #self.nb_examples = 1331\n",
    "            self.nb_examples = 10\n",
    "        else:\n",
    "            raise (\"Dataset: Invalid split. \"\n",
    "                   \"Must be [train, valid, test, train_overlapped, valid_overlapped]\")\n",
    "\n",
    "        filename_x = os.path.join(data_dir, \"{}_x.dat\".format(split))\n",
    "        filename_y = os.path.join(data_dir, \"{}_y.txt\".format(split))\n",
    "\n",
    "        filename_region_ids = os.path.join(data_dir,\n",
    "                                           \"{}_regions_id.txt\".format(split))\n",
    "        self.region_ids = np.loadtxt(filename_region_ids, dtype=object)\n",
    "\n",
    "        self.targets = None\n",
    "        if os.path.exists(filename_y) and not split.startswith(\"train\"):\n",
    "            pre_targets = np.loadtxt(filename_y, 'U2')\n",
    "\n",
    "            if subset is None:\n",
    "                pre_targets = pre_targets[skip: None]\n",
    "            else:\n",
    "                pre_targets = pre_targets[skip: skip + subset]\n",
    "\n",
    "            self.map_labels = np.unique(pre_targets)\n",
    "\n",
    "            self.targets = np.asarray([\n",
    "                np.where(self.map_labels == t)[0][0]\n",
    "                for t in pre_targets\n",
    "            ])\n",
    "        print(self.nb_examples)\n",
    "        print(height)\n",
    "        print(width)\n",
    "        print(nb_channels)\n",
    "        self.data = np.memmap(\n",
    "            filename_x,\n",
    "            dtype=datatype,\n",
    "            mode=\"r\",\n",
    "            shape=(self.nb_examples, height, width, nb_channels)\n",
    "        )\n",
    "\n",
    "        if subset is None:\n",
    "            self.data = self.data[skip: None]\n",
    "            self.region_ids = self.region_ids[skip: None]\n",
    "        else:\n",
    "            self.data = self.data[skip: skip + subset]\n",
    "            self.region_ids = self.region_ids[skip: skip + subset]\n",
    "\n",
    "        self.flattened = flattened\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index]\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        if self.flattened:\n",
    "            img = img.view(-1)\n",
    "\n",
    "        if self.targets is not None:\n",
    "            return img, torch.Tensor([self.targets[index]])\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'cnnautoencoder_cluster_kmeans_l20', 'n_gpu': 1, 'numpy_seed': 1, 'torch_seed': 1, 'wall_time': 8, 'model': {'type': 'ConvolutionalAutoEncoder', 'args': {'code_size': 20, 'dropout': 0.1, 'cnn1_out_channels': 10, 'cnn1_kernel_size': 5, 'cnn2_out_channels': 20, 'cnn2_kernel_size': 5, 'lin2_in_channels': 50, 'maxpool_kernel': 2, 'loss_fct': 'MSELoss'}}, 'optimizer': {'type': 'Adam', 'args': {'lr': 0.0001, 'weight_decay': 0, 'amsgrad': False}}, 'trainer': {'epochs': 100, 'save_dir': 'saved/', 'log_dir': 'logs/', 'save_period': 10, 'type': 'ClusterKMeansTrainer', 'options': {'n_clusters': 100, 'kmeans_interval': 0, 'kmeans_headstart': 0, 'kmeans_weight': 10.0}}, 'data': {'dataset': {'data_dir': '/rap/jvb-000-aa/COURS2019/etudiants/data/horoma', 'flattened': False}, 'dataloader': {'split': 0.9, 'train': {'batch_size': 128, 'shuffle': True}, 'valid': {'batch_size': 128, 'shuffle': False}}}}\n"
     ]
    }
   ],
   "source": [
    "config = json.load(open('../configs/cnnautoencoder_clusters_kmeans.json'))\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: There's no GPU available on this machine, training will be performed on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "32\n",
      "32\n",
      "3\n",
      "10\n",
      "32\n",
      "32\n",
      "3\n",
      "ConvolutionalAutoEncoder(\n",
      "  (loss_fct): MSELoss()\n",
      "  (dropout): Dropout(p=0.1)\n",
      "  (encode_cnn_1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (encode_cnn_2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (encode_lin_1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (encode_lin_2): Linear(in_features=50, out_features=20, bias=True)\n",
      "  (decode_lin_1): Linear(in_features=20, out_features=500, bias=True)\n",
      "  (decode_lin_2): Linear(in_features=500, out_features=3072, bias=True)\n",
      ")\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integeral value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d0fbafe275ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run on a subsample of horoma dataset train_overlapped and valid_overlapped (only take the 5first sampless)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-f9f4ec10a537>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(config, resume, test_run, helios_run, horoma_test)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mlabelled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabelled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mhelios_run\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhelios_run\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trainer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'options'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rap/jvb-000-aa/COURS2019/etudiants/user11/Block3/ift6759-horoma/trainer/cluster_kmeans_trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, optimizer, resume, config, unlabelled, labelled, helios_run, experiment_folder, n_clusters, kmeans_interval, kmeans_headstart, kmeans_weight)\u001b[0m\n\u001b[1;32m     41\u001b[0m                                                    \u001b[0mlabelled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                                                    \u001b[0mhelios_run\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                                                    experiment_folder)\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         self.kmeans = ClusterModel(\n",
      "\u001b[0;32m/rap/jvb-000-aa/COURS2019/etudiants/user11/Block3/ift6759-horoma/trainer/cluster_trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, optimizer, resume, config, unlabelled, labelled, helios_run, experiment_folder)\u001b[0m\n\u001b[1;32m     30\u001b[0m         super(ClusterTrainer, self).__init__(model, optimizer, resume, config,\n\u001b[1;32m     31\u001b[0m                                              \u001b[0munlabelled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelios_run\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                                              experiment_folder)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0msplitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSplitDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/rap/jvb-000-aa/COURS2019/etudiants/user11/Block3/ift6759-horoma/trainer/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, optimizer, resume, config, unlabelled, helios_run, experiment_folder, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataloader'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         )\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn)\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/lib/python3.6/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             raise ValueError(\"num_samples should be a positive integeral \"\n\u001b[0;32m---> 64\u001b[0;31m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplacement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"replacement should be a boolean value, but got \"\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integeral value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# Run on a subsample of horoma dataset train_overlapped and valid_overlapped (only take the 5first sampless)\n",
    "main(config, None, True, None, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
